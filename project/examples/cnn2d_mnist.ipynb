{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of 2D CNN on MNIST data (numpy implementation)\n",
    "Wei Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path\n",
    "# If running the .py script, uncomment the next line\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
    "# add parent directory: adds the parent directory of the module requiring it (__file__)\n",
    "# to the beginning of the module search path.\n",
    "\n",
    "# change the working directory to the parent folder\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_data_utils\n",
    "from utils import data_processor\n",
    "from models.cnn import *\n",
    "from nn.modules.loss import *\n",
    "from nn.modules.activation import *\n",
    "from nn.modules.linear import *\n",
    "from nn.modules.initializer import *\n",
    "from optim.sgd import *\n",
    "from optim.adam import *\n",
    "from evaluation.multiclass_eval import *\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random_seed = 123\n",
    "os.environ[\"PL_GLOBAL_SEED\"] = str(random_seed)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Wei Li\n",
      "\n",
      "Last updated: 2023-12-11 22:47:20\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.17\n",
      "IPython version      : 8.12.2\n",
      "\n",
      "numpy      : 1.21.5\n",
      "torch      : 1.12.1\n",
      "torchvision: 0.13.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pip install watermark\n",
    "%load_ext watermark\n",
    "%watermark -a \"Wei Li\" -u -t -d -v -p numpy,torch,torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MNIST DATASET (Numpy data)\n",
    "##########################\n",
    "\n",
    "transformCompose = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# obtain MNIST numpy files\n",
    "dataset = get_data_utils.get_np_mnist(\n",
    "    validation_fraction=0.2,\n",
    "    train_transforms=transformCompose,\n",
    "    test_transforms=transformCompose,\n",
    ")\n",
    "\n",
    "train_x = dataset[0]  # shape (size , 1, 32, 32), value in [-1, 1]\n",
    "train_y = dataset[1]  # shape (size , 1)\n",
    "valid_x = dataset[2]\n",
    "valid_y = dataset[3]\n",
    "test_x = dataset[4]\n",
    "test_y = dataset[5]\n",
    "\n",
    "\n",
    "# process data targets\n",
    "train_y = data_processor.to_onehot(train_y, 10)\n",
    "valid_y = data_processor.to_onehot(valid_y, 10)\n",
    "test_y = data_processor.to_onehot(test_y, 10)\n",
    "\n",
    "train_x.dtype, train_x.shape  # (48000, 1, 32, 32)\n",
    "train_y.dtype, train_y.shape  # (48000, 10)\n",
    "\n",
    "\n",
    "# train, validation, test data\n",
    "train_data = [train_x, train_y]\n",
    "valid_data = [valid_x, valid_y]\n",
    "test_data = [test_x, test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape, min, max, mean and std: train_x\n",
      "(48000, 1, 32, 32) -1.0 1.0 -0.73777825 0.5792735\n",
      "min, max, mean and std: test_x\n",
      "(10000, 1, 32, 32) -1.0 1.0 -0.7345314 0.5838259\n"
     ]
    }
   ],
   "source": [
    "print(\"shape, min, max, mean and std: train_x\")\n",
    "print(train_x.shape, np.min(train_x), np.max(train_x), np.mean(train_x), np.std(train_x))\n",
    "print(\"min, max, mean and std: test_x\")\n",
    "print(test_x.shape, np.min(test_x), np.max(test_x), np.mean(test_x), np.std(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "The model architecture:\n",
      "layer0:\n",
      "\tsublayer0: <nn.modules.conv.Conv2D object at 0x15aa1f6d0>\n",
      "\tsublayer1: <nn.modules.activation.Tanh object at 0x15aa1fb50>\n",
      "layer1:\n",
      "\tsublayer0: <nn.modules.conv.Pool2D object at 0x15aa1f880>\n",
      "layer2:\n",
      "\tsublayer0: <nn.modules.conv.Conv2D object at 0x15aa1f9a0>\n",
      "\tsublayer1: <nn.modules.activation.Tanh object at 0x15aa1fa90>\n",
      "layer3:\n",
      "\tsublayer0: <nn.modules.conv.Pool2D object at 0x15a8b2730>\n",
      "layer4:\n",
      "\tsublayer0: <nn.modules.conv.Flatten2D object at 0x15aa25910>\n",
      "layer5:\n",
      "\tsublayer0: <nn.modules.linear.Linear object at 0x15aa25940>\n",
      "\tsublayer1: <nn.modules.activation.Tanh object at 0x15aa1fac0>\n",
      "layer6:\n",
      "\tsublayer0: <nn.modules.linear.Linear object at 0x15aa257f0>\n",
      "\tsublayer1: <nn.modules.activation.Tanh object at 0x15aa1fc10>\n",
      "layer7:\n",
      "\tsublayer0: <nn.modules.linear.Linear object at 0x15a8ef6a0>\n",
      "\tsublayer1: <nn.modules.activation.Identity object at 0x15aa1fe80>\n",
      "\n",
      "---------------------------------\n",
      "layers with learnable parameters:\n",
      "layer0 \n",
      " (0)conv1d\n",
      "(6, 1, 5, 5)\n",
      "(6, 1)\n",
      "\n",
      "layer2 \n",
      " (0)conv1d\n",
      "(16, 6, 5, 5)\n",
      "(16, 1)\n",
      "\n",
      "layer5 \n",
      " (0)linear\n",
      "(120, 400)\n",
      "(120, 1)\n",
      "\n",
      "layer6 \n",
      " (0)linear\n",
      "(84, 120)\n",
      "(84, 1)\n",
      "\n",
      "layer7 \n",
      " (0)linear\n",
      "(10, 84)\n",
      "(10, 1)\n",
      "\n",
      "layer0 \n",
      " (0)conv1d\n",
      "(6, 1, 5, 5)\n",
      "(6, 1)\n",
      "\n",
      "layer2 \n",
      " (0)conv1d\n",
      "(16, 6, 5, 5)\n",
      "(16, 1)\n",
      "\n",
      "layer5 \n",
      " (0)linear\n",
      "(120, 400)\n",
      "(120, 1)\n",
      "\n",
      "layer6 \n",
      " (0)linear\n",
      "(84, 120)\n",
      "(84, 1)\n",
      "\n",
      "layer7 \n",
      " (0)linear\n",
      "(10, 84)\n",
      "(10, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "##### Lenet5  ##########\n",
    "########################\n",
    "\n",
    "# Lenet 5 (2D CNN)\n",
    "\n",
    "# x (batch_size, in_channels=1, in_height=32, in_width=32)\n",
    "# -> Conv2d (out_channels 6, kernel 5, stride 1)-> tanh \n",
    "# -> maxpool (kernel 2)\n",
    "# -> Conv2d (out_channels 16, kernel 5, stride 1)-> tanh \n",
    "# -> maxpool (kernel 2)\n",
    "# -> Flatten2D\n",
    "# -> Linear (in_features=16*5*5, out_features=120) -> tanh\n",
    "# -> Linear (in_features=120, out_features=84) -> tanh\n",
    "# -> Linear (in_features=84, out_features=10) (-->identity)\n",
    "\n",
    "# ---- set up the user-defined model  ---- #\n",
    "input_dims = list(train_x.shape[2:4])\n",
    "num_input_channels = train_x.shape[1]\n",
    "\n",
    "out_channels = [6, 16]\n",
    "kernel_sizes = [5, 5]\n",
    "strides = [1, 1]\n",
    "pool_kernel_sizes = [2, 2]\n",
    "conv_activations = [Tanh(), Tanh()]\n",
    "num_linear_neurons = [120, 84, 10]\n",
    "linear_activations = [Tanh(), Tanh(), Identity()]\n",
    "# The last activation is the activation that produces output\n",
    "# we use identity because the CrossEntropyLoss() we use here\n",
    "# is taking logits\n",
    "\n",
    "conv_weight_init_fn = weight_init_He_CNN\n",
    "linear_weight_init_fn = weight_init_He\n",
    "bias_init_fn = bias_init_zeros\n",
    "criterion = CrossEntropyLoss()\n",
    "lr = 1e-1\n",
    "\n",
    "\n",
    "lenet5_model = Lenet5(\n",
    "    input_dims,\n",
    "    num_input_channels,\n",
    "    out_channels,\n",
    "    kernel_sizes,\n",
    "    strides,\n",
    "    num_linear_neurons,\n",
    "    conv_activations,\n",
    "    linear_activations,\n",
    "    conv_weight_init_fn,\n",
    "    bias_init_fn,\n",
    "    linear_weight_init_fn,\n",
    "    pool_kernel_sizes,\n",
    "    pool_mode=\"max\",\n",
    ")\n",
    "\n",
    "type(lenet5_model)\n",
    "\n",
    "# check some utility functions\n",
    "lenet5_model.print_structure()\n",
    "\n",
    "#print_dict(lenet5_model.layers_dict)\n",
    "#print_keys(lenet5_model.paras_dict)\n",
    "\n",
    "model_paras_list = lenet5_model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Batch 000~511/48000 | Train loss: 2.3230\n",
      "Epoch: 001/015 | Batch 512~1023/48000 | Train loss: 2.7377\n",
      "Epoch: 001/015 | Batch 1024~1535/48000 | Train loss: 2.5007\n",
      "Epoch: 001/015 | Batch 1536~2047/48000 | Train loss: 2.2399\n",
      "Epoch: 001/015 | Batch 2048~2559/48000 | Train loss: 2.1752\n",
      "Epoch: 001/015 | Batch 2560~3071/48000 | Train loss: 1.9410\n",
      "Epoch: 001/015 | Batch 3072~3583/48000 | Train loss: 1.8285\n",
      "Epoch: 001/015 | Batch 3584~4095/48000 | Train loss: 1.7994\n",
      "Epoch: 001/015 | Batch 4096~4607/48000 | Train loss: 1.6368\n",
      "Epoch: 001/015 | Batch 4608~5119/48000 | Train loss: 1.7257\n",
      "Epoch: 001/015 | Batch 5120~5631/48000 | Train loss: 1.5118\n",
      "Epoch: 001/015 | Batch 5632~6143/48000 | Train loss: 1.4610\n",
      "Epoch: 001/015 | Batch 6144~6655/48000 | Train loss: 1.2996\n",
      "Epoch: 001/015 | Batch 6656~7167/48000 | Train loss: 1.2094\n",
      "Epoch: 001/015 | Batch 7168~7679/48000 | Train loss: 1.1296\n",
      "Epoch: 001/015 | Batch 7680~8191/48000 | Train loss: 1.1317\n",
      "Epoch: 001/015 | Batch 8192~8703/48000 | Train loss: 1.0127\n",
      "Epoch: 001/015 | Batch 8704~9215/48000 | Train loss: 0.9770\n",
      "Epoch: 001/015 | Batch 9216~9727/48000 | Train loss: 0.9095\n",
      "Epoch: 001/015 | Batch 9728~10239/48000 | Train loss: 0.9494\n",
      "Epoch: 001/015 | Batch 10240~10751/48000 | Train loss: 1.8123\n",
      "Epoch: 001/015 | Batch 10752~11263/48000 | Train loss: 1.6440\n",
      "Epoch: 001/015 | Batch 11264~11775/48000 | Train loss: 1.0492\n",
      "Epoch: 001/015 | Batch 11776~12287/48000 | Train loss: 0.9796\n",
      "Epoch: 001/015 | Batch 12288~12799/48000 | Train loss: 0.8503\n",
      "Epoch: 001/015 | Batch 12800~13311/48000 | Train loss: 0.7992\n",
      "Epoch: 001/015 | Batch 13312~13823/48000 | Train loss: 0.8007\n",
      "Epoch: 001/015 | Batch 13824~14335/48000 | Train loss: 0.8060\n",
      "Epoch: 001/015 | Batch 14336~14847/48000 | Train loss: 0.7687\n",
      "Epoch: 001/015 | Batch 14848~15359/48000 | Train loss: 0.8694\n",
      "Epoch: 001/015 | Batch 15360~15871/48000 | Train loss: 1.0102\n",
      "Epoch: 001/015 | Batch 15872~16383/48000 | Train loss: 0.7466\n",
      "Epoch: 001/015 | Batch 16384~16895/48000 | Train loss: 0.7130\n",
      "Epoch: 001/015 | Batch 16896~17407/48000 | Train loss: 0.7652\n",
      "Epoch: 001/015 | Batch 17408~17919/48000 | Train loss: 0.7724\n",
      "Epoch: 001/015 | Batch 17920~18431/48000 | Train loss: 0.8486\n",
      "Epoch: 001/015 | Batch 18432~18943/48000 | Train loss: 0.7039\n",
      "Epoch: 001/015 | Batch 18944~19455/48000 | Train loss: 0.7162\n",
      "Epoch: 001/015 | Batch 19456~19967/48000 | Train loss: 0.7034\n",
      "Epoch: 001/015 | Batch 19968~20479/48000 | Train loss: 0.6867\n",
      "Epoch: 001/015 | Batch 20480~20991/48000 | Train loss: 0.7082\n",
      "Epoch: 001/015 | Batch 20992~21503/48000 | Train loss: 0.7222\n",
      "Epoch: 001/015 | Batch 21504~22015/48000 | Train loss: 0.8000\n",
      "Epoch: 001/015 | Batch 22016~22527/48000 | Train loss: 0.7275\n",
      "Epoch: 001/015 | Batch 22528~23039/48000 | Train loss: 0.6557\n",
      "Epoch: 001/015 | Batch 23040~23551/48000 | Train loss: 0.5878\n",
      "Epoch: 001/015 | Batch 23552~24063/48000 | Train loss: 0.5714\n",
      "Epoch: 001/015 | Batch 24064~24575/48000 | Train loss: 0.6307\n",
      "Epoch: 001/015 | Batch 24576~25087/48000 | Train loss: 0.6410\n",
      "Epoch: 001/015 | Batch 25088~25599/48000 | Train loss: 0.6912\n",
      "Epoch: 001/015 | Batch 25600~26111/48000 | Train loss: 0.6415\n",
      "Epoch: 001/015 | Batch 26112~26623/48000 | Train loss: 0.5598\n",
      "Epoch: 001/015 | Batch 26624~27135/48000 | Train loss: 0.5496\n",
      "Epoch: 001/015 | Batch 27136~27647/48000 | Train loss: 0.5243\n",
      "Epoch: 001/015 | Batch 27648~28159/48000 | Train loss: 0.5097\n",
      "Epoch: 001/015 | Batch 28160~28671/48000 | Train loss: 0.5250\n",
      "Epoch: 001/015 | Batch 28672~29183/48000 | Train loss: 0.4903\n",
      "Epoch: 001/015 | Batch 29184~29695/48000 | Train loss: 0.5050\n",
      "Epoch: 001/015 | Batch 29696~30207/48000 | Train loss: 0.5194\n",
      "Epoch: 001/015 | Batch 30208~30719/48000 | Train loss: 0.4868\n",
      "Epoch: 001/015 | Batch 30720~31231/48000 | Train loss: 0.5194\n",
      "Epoch: 001/015 | Batch 31232~31743/48000 | Train loss: 0.4709\n",
      "Epoch: 001/015 | Batch 31744~32255/48000 | Train loss: 0.5826\n",
      "Epoch: 001/015 | Batch 32256~32767/48000 | Train loss: 0.5635\n",
      "Epoch: 001/015 | Batch 32768~33279/48000 | Train loss: 0.4499\n",
      "Epoch: 001/015 | Batch 33280~33791/48000 | Train loss: 0.5103\n",
      "Epoch: 001/015 | Batch 33792~34303/48000 | Train loss: 0.4762\n",
      "Epoch: 001/015 | Batch 34304~34815/48000 | Train loss: 0.4252\n",
      "Epoch: 001/015 | Batch 34816~35327/48000 | Train loss: 0.4380\n",
      "Epoch: 001/015 | Batch 35328~35839/48000 | Train loss: 0.3946\n",
      "Epoch: 001/015 | Batch 35840~36351/48000 | Train loss: 0.4705\n",
      "Epoch: 001/015 | Batch 36352~36863/48000 | Train loss: 0.4500\n",
      "Epoch: 001/015 | Batch 36864~37375/48000 | Train loss: 0.4423\n",
      "Epoch: 001/015 | Batch 37376~37887/48000 | Train loss: 0.4554\n",
      "Epoch: 001/015 | Batch 37888~38399/48000 | Train loss: 0.5503\n",
      "Epoch: 001/015 | Batch 38400~38911/48000 | Train loss: 0.4309\n",
      "Epoch: 001/015 | Batch 38912~39423/48000 | Train loss: 0.3745\n",
      "Epoch: 001/015 | Batch 39424~39935/48000 | Train loss: 0.4097\n",
      "Epoch: 001/015 | Batch 39936~40447/48000 | Train loss: 0.4461\n",
      "Epoch: 001/015 | Batch 40448~40959/48000 | Train loss: 0.3693\n",
      "Epoch: 001/015 | Batch 40960~41471/48000 | Train loss: 0.4822\n",
      "Epoch: 001/015 | Batch 41472~41983/48000 | Train loss: 0.4415\n",
      "Epoch: 001/015 | Batch 41984~42495/48000 | Train loss: 0.4631\n",
      "Epoch: 001/015 | Batch 42496~43007/48000 | Train loss: 0.4243\n",
      "Epoch: 001/015 | Batch 43008~43519/48000 | Train loss: 0.3471\n",
      "Epoch: 001/015 | Batch 43520~44031/48000 | Train loss: 0.4233\n",
      "Epoch: 001/015 | Batch 44032~44543/48000 | Train loss: 0.4023\n",
      "Epoch: 001/015 | Batch 44544~45055/48000 | Train loss: 0.4095\n",
      "Epoch: 001/015 | Batch 45056~45567/48000 | Train loss: 0.3841\n",
      "Epoch: 001/015 | Batch 45568~46079/48000 | Train loss: 0.3480\n",
      "Epoch: 001/015 | Batch 46080~46591/48000 | Train loss: 0.4326\n",
      "Epoch: 001/015 | Batch 46592~47103/48000 | Train loss: 0.3858\n",
      "Epoch: 001/015 | Batch 47104~47615/48000 | Train loss: 0.3319\n",
      "Epoch: 001/015 | Batch 47616~48127/48000 | Train loss: 0.4270\n",
      "Epoch: 001/015 | Batch 000~511/12000 | Validation loss: 0.3534\n",
      "Epoch: 001/015 | Batch 512~1023/12000 | Validation loss: 0.3719\n",
      "Epoch: 001/015 | Batch 1024~1535/12000 | Validation loss: 0.3592\n",
      "Epoch: 001/015 | Batch 1536~2047/12000 | Validation loss: 0.3765\n",
      "Epoch: 001/015 | Batch 2048~2559/12000 | Validation loss: 0.3927\n",
      "Epoch: 001/015 | Batch 2560~3071/12000 | Validation loss: 0.3707\n",
      "Epoch: 001/015 | Batch 3072~3583/12000 | Validation loss: 0.3610\n",
      "Epoch: 001/015 | Batch 3584~4095/12000 | Validation loss: 0.4134\n",
      "Epoch: 001/015 | Batch 4096~4607/12000 | Validation loss: 0.3449\n",
      "Epoch: 001/015 | Batch 4608~5119/12000 | Validation loss: 0.3732\n",
      "Epoch: 001/015 | Batch 5120~5631/12000 | Validation loss: 0.4475\n",
      "Epoch: 001/015 | Batch 5632~6143/12000 | Validation loss: 0.3992\n",
      "Epoch: 001/015 | Batch 6144~6655/12000 | Validation loss: 0.4077\n",
      "Epoch: 001/015 | Batch 6656~7167/12000 | Validation loss: 0.4135\n",
      "Epoch: 001/015 | Batch 7168~7679/12000 | Validation loss: 0.4339\n",
      "Epoch: 001/015 | Batch 7680~8191/12000 | Validation loss: 0.3706\n",
      "Epoch: 001/015 | Batch 8192~8703/12000 | Validation loss: 0.3801\n",
      "Epoch: 001/015 | Batch 8704~9215/12000 | Validation loss: 0.3787\n",
      "Epoch: 001/015 | Batch 9216~9727/12000 | Validation loss: 0.3564\n",
      "Epoch: 001/015 | Batch 9728~10239/12000 | Validation loss: 0.3449\n",
      "Epoch: 001/015 | Batch 10240~10751/12000 | Validation loss: 0.3993\n",
      "Epoch: 001/015 | Batch 10752~11263/12000 | Validation loss: 0.3933\n",
      "Epoch: 001/015 | Batch 11264~11775/12000 | Validation loss: 0.3549\n",
      "Epoch: 001/015 | Batch 11776~12287/12000 | Validation loss: 0.3867\n",
      "Epoch: 001/015 | Train loss: 0.8247 | Validation loss: 0.3826 \n",
      "Epoch: 001/015 | Train error: 0.2496 | Validation error: 0.1110 \n",
      "Time elapsed: 51.38 min\n",
      "Epoch: 002/015 | Batch 000~511/48000 | Train loss: 0.3998\n",
      "Epoch: 002/015 | Batch 512~1023/48000 | Train loss: 0.3631\n",
      "Epoch: 002/015 | Batch 1024~1535/48000 | Train loss: 0.3963\n",
      "Epoch: 002/015 | Batch 1536~2047/48000 | Train loss: 0.3485\n",
      "Epoch: 002/015 | Batch 2048~2559/48000 | Train loss: 0.4393\n",
      "Epoch: 002/015 | Batch 2560~3071/48000 | Train loss: 0.3701\n",
      "Epoch: 002/015 | Batch 3072~3583/48000 | Train loss: 0.4116\n",
      "Epoch: 002/015 | Batch 3584~4095/48000 | Train loss: 0.3943\n",
      "Epoch: 002/015 | Batch 4096~4607/48000 | Train loss: 0.3391\n",
      "Epoch: 002/015 | Batch 4608~5119/48000 | Train loss: 0.3444\n",
      "Epoch: 002/015 | Batch 5120~5631/48000 | Train loss: 0.2754\n",
      "Epoch: 002/015 | Batch 5632~6143/48000 | Train loss: 0.3212\n",
      "Epoch: 002/015 | Batch 6144~6655/48000 | Train loss: 0.3771\n",
      "Epoch: 002/015 | Batch 6656~7167/48000 | Train loss: 0.3126\n",
      "Epoch: 002/015 | Batch 7168~7679/48000 | Train loss: 0.3447\n",
      "Epoch: 002/015 | Batch 7680~8191/48000 | Train loss: 0.3666\n",
      "Epoch: 002/015 | Batch 8192~8703/48000 | Train loss: 0.3769\n",
      "Epoch: 002/015 | Batch 8704~9215/48000 | Train loss: 0.3452\n",
      "Epoch: 002/015 | Batch 9216~9727/48000 | Train loss: 0.3294\n",
      "Epoch: 002/015 | Batch 9728~10239/48000 | Train loss: 0.3269\n",
      "Epoch: 002/015 | Batch 10240~10751/48000 | Train loss: 0.3439\n",
      "Epoch: 002/015 | Batch 10752~11263/48000 | Train loss: 0.3134\n",
      "Epoch: 002/015 | Batch 11264~11775/48000 | Train loss: 0.3796\n",
      "Epoch: 002/015 | Batch 11776~12287/48000 | Train loss: 0.3558\n",
      "Epoch: 002/015 | Batch 12288~12799/48000 | Train loss: 0.3171\n",
      "Epoch: 002/015 | Batch 12800~13311/48000 | Train loss: 0.2933\n",
      "Epoch: 002/015 | Batch 13312~13823/48000 | Train loss: 0.3954\n",
      "Epoch: 002/015 | Batch 13824~14335/48000 | Train loss: 0.3348\n",
      "Epoch: 002/015 | Batch 14336~14847/48000 | Train loss: 0.3858\n",
      "Epoch: 002/015 | Batch 14848~15359/48000 | Train loss: 0.3233\n",
      "Epoch: 002/015 | Batch 15360~15871/48000 | Train loss: 0.3318\n",
      "Epoch: 002/015 | Batch 15872~16383/48000 | Train loss: 0.3522\n",
      "Epoch: 002/015 | Batch 16384~16895/48000 | Train loss: 0.3128\n",
      "Epoch: 002/015 | Batch 16896~17407/48000 | Train loss: 0.3801\n",
      "Epoch: 002/015 | Batch 17408~17919/48000 | Train loss: 0.3982\n",
      "Epoch: 002/015 | Batch 17920~18431/48000 | Train loss: 0.3013\n",
      "Epoch: 002/015 | Batch 18432~18943/48000 | Train loss: 0.3336\n",
      "Epoch: 002/015 | Batch 18944~19455/48000 | Train loss: 0.3348\n",
      "Epoch: 002/015 | Batch 19456~19967/48000 | Train loss: 0.3228\n",
      "Epoch: 002/015 | Batch 19968~20479/48000 | Train loss: 0.3033\n",
      "Epoch: 002/015 | Batch 20480~20991/48000 | Train loss: 0.3599\n",
      "Epoch: 002/015 | Batch 20992~21503/48000 | Train loss: 0.3050\n",
      "Epoch: 002/015 | Batch 21504~22015/48000 | Train loss: 0.3532\n",
      "Epoch: 002/015 | Batch 22016~22527/48000 | Train loss: 0.3629\n",
      "Epoch: 002/015 | Batch 22528~23039/48000 | Train loss: 0.2849\n",
      "Epoch: 002/015 | Batch 23040~23551/48000 | Train loss: 0.3564\n",
      "Epoch: 002/015 | Batch 23552~24063/48000 | Train loss: 0.3170\n",
      "Epoch: 002/015 | Batch 24064~24575/48000 | Train loss: 0.4270\n",
      "Epoch: 002/015 | Batch 24576~25087/48000 | Train loss: 0.3193\n",
      "Epoch: 002/015 | Batch 25088~25599/48000 | Train loss: 0.3435\n",
      "Epoch: 002/015 | Batch 25600~26111/48000 | Train loss: 0.3732\n",
      "Epoch: 002/015 | Batch 26112~26623/48000 | Train loss: 0.3072\n",
      "Epoch: 002/015 | Batch 26624~27135/48000 | Train loss: 0.3206\n",
      "Epoch: 002/015 | Batch 27136~27647/48000 | Train loss: 0.2208\n",
      "Epoch: 002/015 | Batch 27648~28159/48000 | Train loss: 0.3150\n",
      "Epoch: 002/015 | Batch 28160~28671/48000 | Train loss: 0.2751\n",
      "Epoch: 002/015 | Batch 28672~29183/48000 | Train loss: 0.2682\n",
      "Epoch: 002/015 | Batch 29184~29695/48000 | Train loss: 0.3159\n",
      "Epoch: 002/015 | Batch 29696~30207/48000 | Train loss: 0.2738\n",
      "Epoch: 002/015 | Batch 30208~30719/48000 | Train loss: 0.2446\n",
      "Epoch: 002/015 | Batch 30720~31231/48000 | Train loss: 0.3380\n",
      "Epoch: 002/015 | Batch 31232~31743/48000 | Train loss: 0.2547\n",
      "Epoch: 002/015 | Batch 31744~32255/48000 | Train loss: 0.2423\n",
      "Epoch: 002/015 | Batch 32256~32767/48000 | Train loss: 0.3225\n",
      "Epoch: 002/015 | Batch 32768~33279/48000 | Train loss: 0.2977\n",
      "Epoch: 002/015 | Batch 33280~33791/48000 | Train loss: 0.3570\n",
      "Epoch: 002/015 | Batch 33792~34303/48000 | Train loss: 0.2877\n",
      "Epoch: 002/015 | Batch 34304~34815/48000 | Train loss: 0.2923\n",
      "Epoch: 002/015 | Batch 34816~35327/48000 | Train loss: 0.3608\n",
      "Epoch: 002/015 | Batch 35328~35839/48000 | Train loss: 0.3600\n",
      "Epoch: 002/015 | Batch 35840~36351/48000 | Train loss: 0.3387\n",
      "Epoch: 002/015 | Batch 36352~36863/48000 | Train loss: 0.2819\n",
      "Epoch: 002/015 | Batch 36864~37375/48000 | Train loss: 0.2733\n",
      "Epoch: 002/015 | Batch 37376~37887/48000 | Train loss: 0.2903\n",
      "Epoch: 002/015 | Batch 37888~38399/48000 | Train loss: 0.2533\n",
      "Epoch: 002/015 | Batch 38400~38911/48000 | Train loss: 0.3090\n",
      "Epoch: 002/015 | Batch 38912~39423/48000 | Train loss: 0.3079\n",
      "Epoch: 002/015 | Batch 39424~39935/48000 | Train loss: 0.3751\n",
      "Epoch: 002/015 | Batch 39936~40447/48000 | Train loss: 0.2839\n",
      "Epoch: 002/015 | Batch 40448~40959/48000 | Train loss: 0.3514\n",
      "Epoch: 002/015 | Batch 40960~41471/48000 | Train loss: 0.3007\n",
      "Epoch: 002/015 | Batch 41472~41983/48000 | Train loss: 0.2876\n",
      "Epoch: 002/015 | Batch 41984~42495/48000 | Train loss: 0.3427\n",
      "Epoch: 002/015 | Batch 42496~43007/48000 | Train loss: 0.2891\n",
      "Epoch: 002/015 | Batch 43008~43519/48000 | Train loss: 0.3252\n",
      "Epoch: 002/015 | Batch 43520~44031/48000 | Train loss: 0.2896\n",
      "Epoch: 002/015 | Batch 44032~44543/48000 | Train loss: 0.3137\n",
      "Epoch: 002/015 | Batch 44544~45055/48000 | Train loss: 0.2757\n",
      "Epoch: 002/015 | Batch 45056~45567/48000 | Train loss: 0.2824\n",
      "Epoch: 002/015 | Batch 45568~46079/48000 | Train loss: 0.2483\n",
      "Epoch: 002/015 | Batch 46080~46591/48000 | Train loss: 0.2421\n",
      "Epoch: 002/015 | Batch 46592~47103/48000 | Train loss: 0.2838\n",
      "Epoch: 002/015 | Batch 47104~47615/48000 | Train loss: 0.2461\n",
      "Epoch: 002/015 | Batch 47616~48127/48000 | Train loss: 0.3051\n",
      "Epoch: 002/015 | Batch 000~511/12000 | Validation loss: 0.2614\n",
      "Epoch: 002/015 | Batch 512~1023/12000 | Validation loss: 0.2051\n",
      "Epoch: 002/015 | Batch 1024~1535/12000 | Validation loss: 0.2200\n",
      "Epoch: 002/015 | Batch 1536~2047/12000 | Validation loss: 0.2466\n",
      "Epoch: 002/015 | Batch 2048~2559/12000 | Validation loss: 0.2522\n",
      "Epoch: 002/015 | Batch 2560~3071/12000 | Validation loss: 0.2534\n",
      "Epoch: 002/015 | Batch 3072~3583/12000 | Validation loss: 0.2515\n",
      "Epoch: 002/015 | Batch 3584~4095/12000 | Validation loss: 0.2782\n",
      "Epoch: 002/015 | Batch 4096~4607/12000 | Validation loss: 0.2145\n",
      "Epoch: 002/015 | Batch 4608~5119/12000 | Validation loss: 0.2625\n",
      "Epoch: 002/015 | Batch 5120~5631/12000 | Validation loss: 0.3198\n",
      "Epoch: 002/015 | Batch 5632~6143/12000 | Validation loss: 0.2785\n",
      "Epoch: 002/015 | Batch 6144~6655/12000 | Validation loss: 0.2660\n",
      "Epoch: 002/015 | Batch 6656~7167/12000 | Validation loss: 0.2906\n",
      "Epoch: 002/015 | Batch 7168~7679/12000 | Validation loss: 0.2956\n",
      "Epoch: 002/015 | Batch 7680~8191/12000 | Validation loss: 0.2352\n",
      "Epoch: 002/015 | Batch 8192~8703/12000 | Validation loss: 0.2635\n",
      "Epoch: 002/015 | Batch 8704~9215/12000 | Validation loss: 0.2562\n",
      "Epoch: 002/015 | Batch 9216~9727/12000 | Validation loss: 0.2346\n",
      "Epoch: 002/015 | Batch 9728~10239/12000 | Validation loss: 0.2137\n",
      "Epoch: 002/015 | Batch 10240~10751/12000 | Validation loss: 0.2834\n",
      "Epoch: 002/015 | Batch 10752~11263/12000 | Validation loss: 0.2456\n",
      "Epoch: 002/015 | Batch 11264~11775/12000 | Validation loss: 0.2261\n",
      "Epoch: 002/015 | Batch 11776~12287/12000 | Validation loss: 0.2776\n",
      "Epoch: 002/015 | Train loss: 0.3250 | Validation loss: 0.2555 \n",
      "Epoch: 002/015 | Train error: 0.0951 | Validation error: 0.0755 \n",
      "Time elapsed: 49.67 min\n",
      "Epoch: 003/015 | Batch 000~511/48000 | Train loss: 0.3092\n",
      "Epoch: 003/015 | Batch 512~1023/48000 | Train loss: 0.2682\n",
      "Epoch: 003/015 | Batch 1024~1535/48000 | Train loss: 0.2650\n",
      "Epoch: 003/015 | Batch 1536~2047/48000 | Train loss: 0.3049\n",
      "Epoch: 003/015 | Batch 2048~2559/48000 | Train loss: 0.2917\n",
      "Epoch: 003/015 | Batch 2560~3071/48000 | Train loss: 0.2882\n",
      "Epoch: 003/015 | Batch 3072~3583/48000 | Train loss: 0.3122\n",
      "Epoch: 003/015 | Batch 3584~4095/48000 | Train loss: 0.3082\n",
      "Epoch: 003/015 | Batch 4096~4607/48000 | Train loss: 0.2639\n",
      "Epoch: 003/015 | Batch 4608~5119/48000 | Train loss: 0.2786\n",
      "Epoch: 003/015 | Batch 5120~5631/48000 | Train loss: 0.2602\n",
      "Epoch: 003/015 | Batch 5632~6143/48000 | Train loss: 0.2951\n",
      "Epoch: 003/015 | Batch 6144~6655/48000 | Train loss: 0.3243\n",
      "Epoch: 003/015 | Batch 6656~7167/48000 | Train loss: 0.2921\n",
      "Epoch: 003/015 | Batch 7168~7679/48000 | Train loss: 0.2395\n",
      "Epoch: 003/015 | Batch 7680~8191/48000 | Train loss: 0.2264\n",
      "Epoch: 003/015 | Batch 8192~8703/48000 | Train loss: 0.2654\n",
      "Epoch: 003/015 | Batch 8704~9215/48000 | Train loss: 0.2240\n",
      "Epoch: 003/015 | Batch 9216~9727/48000 | Train loss: 0.2167\n",
      "Epoch: 003/015 | Batch 9728~10239/48000 | Train loss: 0.2586\n",
      "Epoch: 003/015 | Batch 10240~10751/48000 | Train loss: 0.2386\n",
      "Epoch: 003/015 | Batch 10752~11263/48000 | Train loss: 0.2733\n",
      "Epoch: 003/015 | Batch 11264~11775/48000 | Train loss: 0.2581\n",
      "Epoch: 003/015 | Batch 11776~12287/48000 | Train loss: 0.3136\n",
      "Epoch: 003/015 | Batch 12288~12799/48000 | Train loss: 0.2099\n",
      "Epoch: 003/015 | Batch 12800~13311/48000 | Train loss: 0.2657\n",
      "Epoch: 003/015 | Batch 13312~13823/48000 | Train loss: 0.1721\n",
      "Epoch: 003/015 | Batch 13824~14335/48000 | Train loss: 0.2548\n",
      "Epoch: 003/015 | Batch 14336~14847/48000 | Train loss: 0.2335\n",
      "Epoch: 003/015 | Batch 14848~15359/48000 | Train loss: 0.2379\n",
      "Epoch: 003/015 | Batch 15360~15871/48000 | Train loss: 0.2490\n",
      "Epoch: 003/015 | Batch 15872~16383/48000 | Train loss: 0.2131\n",
      "Epoch: 003/015 | Batch 16384~16895/48000 | Train loss: 0.2218\n",
      "Epoch: 003/015 | Batch 16896~17407/48000 | Train loss: 0.2545\n",
      "Epoch: 003/015 | Batch 17408~17919/48000 | Train loss: 0.2871\n",
      "Epoch: 003/015 | Batch 17920~18431/48000 | Train loss: 0.2130\n",
      "Epoch: 003/015 | Batch 18432~18943/48000 | Train loss: 0.1914\n",
      "Epoch: 003/015 | Batch 18944~19455/48000 | Train loss: 0.2805\n",
      "Epoch: 003/015 | Batch 19456~19967/48000 | Train loss: 0.3067\n",
      "Epoch: 003/015 | Batch 19968~20479/48000 | Train loss: 0.3072\n",
      "Epoch: 003/015 | Batch 20480~20991/48000 | Train loss: 0.2767\n",
      "Epoch: 003/015 | Batch 20992~21503/48000 | Train loss: 0.2169\n",
      "Epoch: 003/015 | Batch 21504~22015/48000 | Train loss: 0.3000\n",
      "Epoch: 003/015 | Batch 22016~22527/48000 | Train loss: 0.2466\n",
      "Epoch: 003/015 | Batch 22528~23039/48000 | Train loss: 0.2733\n",
      "Epoch: 003/015 | Batch 23040~23551/48000 | Train loss: 0.2790\n",
      "Epoch: 003/015 | Batch 23552~24063/48000 | Train loss: 0.2376\n",
      "Epoch: 003/015 | Batch 24064~24575/48000 | Train loss: 0.4304\n",
      "Epoch: 003/015 | Batch 24576~25087/48000 | Train loss: 0.5886\n",
      "Epoch: 003/015 | Batch 25088~25599/48000 | Train loss: 0.4523\n",
      "Epoch: 003/015 | Batch 25600~26111/48000 | Train loss: 0.3590\n",
      "Epoch: 003/015 | Batch 26112~26623/48000 | Train loss: 0.3356\n",
      "Epoch: 003/015 | Batch 26624~27135/48000 | Train loss: 0.3015\n",
      "Epoch: 003/015 | Batch 27136~27647/48000 | Train loss: 0.2705\n",
      "Epoch: 003/015 | Batch 27648~28159/48000 | Train loss: 0.2440\n",
      "Epoch: 003/015 | Batch 28160~28671/48000 | Train loss: 0.2825\n",
      "Epoch: 003/015 | Batch 28672~29183/48000 | Train loss: 0.2622\n",
      "Epoch: 003/015 | Batch 29184~29695/48000 | Train loss: 0.2468\n",
      "Epoch: 003/015 | Batch 29696~30207/48000 | Train loss: 0.2872\n",
      "Epoch: 003/015 | Batch 30208~30719/48000 | Train loss: 0.2388\n",
      "Epoch: 003/015 | Batch 30720~31231/48000 | Train loss: 0.2773\n",
      "Epoch: 003/015 | Batch 31232~31743/48000 | Train loss: 0.2362\n",
      "Epoch: 003/015 | Batch 31744~32255/48000 | Train loss: 0.2627\n",
      "Epoch: 003/015 | Batch 32256~32767/48000 | Train loss: 0.2473\n",
      "Epoch: 003/015 | Batch 32768~33279/48000 | Train loss: 0.2746\n",
      "Epoch: 003/015 | Batch 33280~33791/48000 | Train loss: 0.2329\n",
      "Epoch: 003/015 | Batch 33792~34303/48000 | Train loss: 0.2232\n",
      "Epoch: 003/015 | Batch 34304~34815/48000 | Train loss: 0.2907\n",
      "Epoch: 003/015 | Batch 34816~35327/48000 | Train loss: 0.3282\n",
      "Epoch: 003/015 | Batch 35328~35839/48000 | Train loss: 0.2769\n",
      "Epoch: 003/015 | Batch 35840~36351/48000 | Train loss: 0.2547\n",
      "Epoch: 003/015 | Batch 36352~36863/48000 | Train loss: 0.3199\n",
      "Epoch: 003/015 | Batch 36864~37375/48000 | Train loss: 0.2565\n",
      "Epoch: 003/015 | Batch 37376~37887/48000 | Train loss: 0.2348\n",
      "Epoch: 003/015 | Batch 37888~38399/48000 | Train loss: 0.2305\n",
      "Epoch: 003/015 | Batch 38400~38911/48000 | Train loss: 0.2550\n",
      "Epoch: 003/015 | Batch 38912~39423/48000 | Train loss: 0.2790\n",
      "Epoch: 003/015 | Batch 39424~39935/48000 | Train loss: 0.2331\n",
      "Epoch: 003/015 | Batch 39936~40447/48000 | Train loss: 0.2585\n",
      "Epoch: 003/015 | Batch 40448~40959/48000 | Train loss: 0.2209\n",
      "Epoch: 003/015 | Batch 40960~41471/48000 | Train loss: 0.2289\n",
      "Epoch: 003/015 | Batch 41472~41983/48000 | Train loss: 0.2668\n",
      "Epoch: 003/015 | Batch 41984~42495/48000 | Train loss: 0.2737\n",
      "Epoch: 003/015 | Batch 42496~43007/48000 | Train loss: 0.2612\n",
      "Epoch: 003/015 | Batch 43008~43519/48000 | Train loss: 0.2475\n",
      "Epoch: 003/015 | Batch 43520~44031/48000 | Train loss: 0.2701\n",
      "Epoch: 003/015 | Batch 44032~44543/48000 | Train loss: 0.3409\n",
      "Epoch: 003/015 | Batch 44544~45055/48000 | Train loss: 0.3070\n",
      "Epoch: 003/015 | Batch 45056~45567/48000 | Train loss: 0.4209\n",
      "Epoch: 003/015 | Batch 45568~46079/48000 | Train loss: 0.5400\n",
      "Epoch: 003/015 | Batch 46080~46591/48000 | Train loss: 0.3567\n",
      "Epoch: 003/015 | Batch 46592~47103/48000 | Train loss: 0.3100\n",
      "Epoch: 003/015 | Batch 47104~47615/48000 | Train loss: 0.2546\n",
      "Epoch: 003/015 | Batch 47616~48127/48000 | Train loss: 0.2811\n",
      "Epoch: 003/015 | Batch 000~511/12000 | Validation loss: 0.2281\n",
      "Epoch: 003/015 | Batch 512~1023/12000 | Validation loss: 0.2156\n",
      "Epoch: 003/015 | Batch 1024~1535/12000 | Validation loss: 0.2257\n",
      "Epoch: 003/015 | Batch 1536~2047/12000 | Validation loss: 0.2784\n",
      "Epoch: 003/015 | Batch 2048~2559/12000 | Validation loss: 0.2774\n",
      "Epoch: 003/015 | Batch 2560~3071/12000 | Validation loss: 0.2243\n",
      "Epoch: 003/015 | Batch 3072~3583/12000 | Validation loss: 0.2533\n",
      "Epoch: 003/015 | Batch 3584~4095/12000 | Validation loss: 0.2806\n",
      "Epoch: 003/015 | Batch 4096~4607/12000 | Validation loss: 0.1943\n",
      "Epoch: 003/015 | Batch 4608~5119/12000 | Validation loss: 0.2653\n",
      "Epoch: 003/015 | Batch 5120~5631/12000 | Validation loss: 0.3500\n",
      "Epoch: 003/015 | Batch 5632~6143/12000 | Validation loss: 0.2847\n",
      "Epoch: 003/015 | Batch 6144~6655/12000 | Validation loss: 0.3169\n",
      "Epoch: 003/015 | Batch 6656~7167/12000 | Validation loss: 0.3129\n",
      "Epoch: 003/015 | Batch 7168~7679/12000 | Validation loss: 0.2578\n",
      "Epoch: 003/015 | Batch 7680~8191/12000 | Validation loss: 0.2460\n",
      "Epoch: 003/015 | Batch 8192~8703/12000 | Validation loss: 0.2717\n",
      "Epoch: 003/015 | Batch 8704~9215/12000 | Validation loss: 0.2753\n",
      "Epoch: 003/015 | Batch 9216~9727/12000 | Validation loss: 0.2629\n",
      "Epoch: 003/015 | Batch 9728~10239/12000 | Validation loss: 0.2286\n",
      "Epoch: 003/015 | Batch 10240~10751/12000 | Validation loss: 0.2496\n",
      "Epoch: 003/015 | Batch 10752~11263/12000 | Validation loss: 0.2712\n",
      "Epoch: 003/015 | Batch 11264~11775/12000 | Validation loss: 0.2434\n",
      "Epoch: 003/015 | Batch 11776~12287/12000 | Validation loss: 0.2512\n",
      "Epoch: 003/015 | Train loss: 0.2783 | Validation loss: 0.2611 \n",
      "Epoch: 003/015 | Train error: 0.0859 | Validation error: 0.0801 \n",
      "Time elapsed: 49.32 min\n",
      "Epoch: 004/015 | Batch 000~511/48000 | Train loss: 0.2810\n",
      "Epoch: 004/015 | Batch 512~1023/48000 | Train loss: 0.2428\n",
      "Epoch: 004/015 | Batch 1024~1535/48000 | Train loss: 0.2282\n",
      "Epoch: 004/015 | Batch 1536~2047/48000 | Train loss: 0.2371\n",
      "Epoch: 004/015 | Batch 2048~2559/48000 | Train loss: 0.3201\n",
      "Epoch: 004/015 | Batch 2560~3071/48000 | Train loss: 0.2320\n",
      "Epoch: 004/015 | Batch 3072~3583/48000 | Train loss: 0.2414\n",
      "Epoch: 004/015 | Batch 3584~4095/48000 | Train loss: 0.2631\n",
      "Epoch: 004/015 | Batch 4096~4607/48000 | Train loss: 0.2292\n",
      "Epoch: 004/015 | Batch 4608~5119/48000 | Train loss: 0.2476\n",
      "Epoch: 004/015 | Batch 5120~5631/48000 | Train loss: 0.2210\n",
      "Epoch: 004/015 | Batch 5632~6143/48000 | Train loss: 0.2392\n",
      "Epoch: 004/015 | Batch 6144~6655/48000 | Train loss: 0.2112\n",
      "Epoch: 004/015 | Batch 6656~7167/48000 | Train loss: 0.2205\n",
      "Epoch: 004/015 | Batch 7168~7679/48000 | Train loss: 0.2283\n",
      "Epoch: 004/015 | Batch 7680~8191/48000 | Train loss: 0.2610\n",
      "Epoch: 004/015 | Batch 8192~8703/48000 | Train loss: 0.2257\n",
      "Epoch: 004/015 | Batch 8704~9215/48000 | Train loss: 0.1880\n",
      "Epoch: 004/015 | Batch 9216~9727/48000 | Train loss: 0.2504\n",
      "Epoch: 004/015 | Batch 9728~10239/48000 | Train loss: 0.2331\n",
      "Epoch: 004/015 | Batch 10240~10751/48000 | Train loss: 0.2425\n",
      "Epoch: 004/015 | Batch 10752~11263/48000 | Train loss: 0.2624\n",
      "Epoch: 004/015 | Batch 11264~11775/48000 | Train loss: 0.2507\n",
      "Epoch: 004/015 | Batch 11776~12287/48000 | Train loss: 0.2597\n",
      "Epoch: 004/015 | Batch 12288~12799/48000 | Train loss: 0.2267\n",
      "Epoch: 004/015 | Batch 12800~13311/48000 | Train loss: 0.2326\n",
      "Epoch: 004/015 | Batch 13312~13823/48000 | Train loss: 0.2566\n",
      "Epoch: 004/015 | Batch 13824~14335/48000 | Train loss: 0.2053\n",
      "Epoch: 004/015 | Batch 14336~14847/48000 | Train loss: 0.2128\n",
      "Epoch: 004/015 | Batch 14848~15359/48000 | Train loss: 0.2122\n",
      "Epoch: 004/015 | Batch 15360~15871/48000 | Train loss: 0.1933\n",
      "Epoch: 004/015 | Batch 15872~16383/48000 | Train loss: 0.2118\n",
      "Epoch: 004/015 | Batch 16384~16895/48000 | Train loss: 0.2179\n",
      "Epoch: 004/015 | Batch 16896~17407/48000 | Train loss: 0.1522\n",
      "Epoch: 004/015 | Batch 17408~17919/48000 | Train loss: 0.2204\n",
      "Epoch: 004/015 | Batch 17920~18431/48000 | Train loss: 0.2074\n",
      "Epoch: 004/015 | Batch 18432~18943/48000 | Train loss: 0.2386\n",
      "Epoch: 004/015 | Batch 18944~19455/48000 | Train loss: 0.2135\n",
      "Epoch: 004/015 | Batch 19456~19967/48000 | Train loss: 0.1733\n",
      "Epoch: 004/015 | Batch 19968~20479/48000 | Train loss: 0.1922\n",
      "Epoch: 004/015 | Batch 20480~20991/48000 | Train loss: 0.2000\n",
      "Epoch: 004/015 | Batch 20992~21503/48000 | Train loss: 0.2256\n",
      "Epoch: 004/015 | Batch 21504~22015/48000 | Train loss: 0.1934\n",
      "Epoch: 004/015 | Batch 22016~22527/48000 | Train loss: 0.1606\n",
      "Epoch: 004/015 | Batch 22528~23039/48000 | Train loss: 0.2011\n",
      "Epoch: 004/015 | Batch 23040~23551/48000 | Train loss: 0.2173\n",
      "Epoch: 004/015 | Batch 23552~24063/48000 | Train loss: 0.2269\n",
      "Epoch: 004/015 | Batch 24064~24575/48000 | Train loss: 0.2326\n",
      "Epoch: 004/015 | Batch 24576~25087/48000 | Train loss: 0.1990\n",
      "Epoch: 004/015 | Batch 25088~25599/48000 | Train loss: 0.2017\n",
      "Epoch: 004/015 | Batch 25600~26111/48000 | Train loss: 0.2169\n",
      "Epoch: 004/015 | Batch 26112~26623/48000 | Train loss: 0.2221\n",
      "Epoch: 004/015 | Batch 26624~27135/48000 | Train loss: 0.1986\n",
      "Epoch: 004/015 | Batch 27136~27647/48000 | Train loss: 0.2624\n",
      "Epoch: 004/015 | Batch 27648~28159/48000 | Train loss: 0.1706\n",
      "Epoch: 004/015 | Batch 28160~28671/48000 | Train loss: 0.1789\n",
      "Epoch: 004/015 | Batch 28672~29183/48000 | Train loss: 0.2470\n",
      "Epoch: 004/015 | Batch 29184~29695/48000 | Train loss: 0.2110\n",
      "Epoch: 004/015 | Batch 29696~30207/48000 | Train loss: 0.2457\n",
      "Epoch: 004/015 | Batch 30208~30719/48000 | Train loss: 0.2450\n",
      "Epoch: 004/015 | Batch 30720~31231/48000 | Train loss: 0.2148\n",
      "Epoch: 004/015 | Batch 31232~31743/48000 | Train loss: 0.1926\n",
      "Epoch: 004/015 | Batch 31744~32255/48000 | Train loss: 0.2151\n",
      "Epoch: 004/015 | Batch 32256~32767/48000 | Train loss: 0.2058\n",
      "Epoch: 004/015 | Batch 32768~33279/48000 | Train loss: 0.2165\n",
      "Epoch: 004/015 | Batch 33280~33791/48000 | Train loss: 0.2624\n",
      "Epoch: 004/015 | Batch 33792~34303/48000 | Train loss: 0.2288\n",
      "Epoch: 004/015 | Batch 34304~34815/48000 | Train loss: 0.2080\n",
      "Epoch: 004/015 | Batch 34816~35327/48000 | Train loss: 0.2215\n",
      "Epoch: 004/015 | Batch 35328~35839/48000 | Train loss: 0.2460\n",
      "Epoch: 004/015 | Batch 35840~36351/48000 | Train loss: 0.2190\n",
      "Epoch: 004/015 | Batch 36352~36863/48000 | Train loss: 0.2558\n",
      "Epoch: 004/015 | Batch 36864~37375/48000 | Train loss: 0.1975\n",
      "Epoch: 004/015 | Batch 37376~37887/48000 | Train loss: 0.4902\n",
      "Epoch: 004/015 | Batch 37888~38399/48000 | Train loss: 0.5095\n",
      "Epoch: 004/015 | Batch 38400~38911/48000 | Train loss: 0.4710\n",
      "Epoch: 004/015 | Batch 38912~39423/48000 | Train loss: 0.2733\n",
      "Epoch: 004/015 | Batch 39424~39935/48000 | Train loss: 0.2803\n",
      "Epoch: 004/015 | Batch 39936~40447/48000 | Train loss: 0.2841\n",
      "Epoch: 004/015 | Batch 40448~40959/48000 | Train loss: 0.2628\n",
      "Epoch: 004/015 | Batch 40960~41471/48000 | Train loss: 0.2336\n",
      "Epoch: 004/015 | Batch 41472~41983/48000 | Train loss: 0.2754\n",
      "Epoch: 004/015 | Batch 41984~42495/48000 | Train loss: 0.2541\n",
      "Epoch: 004/015 | Batch 42496~43007/48000 | Train loss: 0.2299\n",
      "Epoch: 004/015 | Batch 43008~43519/48000 | Train loss: 0.1476\n",
      "Epoch: 004/015 | Batch 43520~44031/48000 | Train loss: 0.2004\n",
      "Epoch: 004/015 | Batch 44032~44543/48000 | Train loss: 0.2342\n",
      "Epoch: 004/015 | Batch 44544~45055/48000 | Train loss: 0.1586\n",
      "Epoch: 004/015 | Batch 45056~45567/48000 | Train loss: 0.2252\n",
      "Epoch: 004/015 | Batch 45568~46079/48000 | Train loss: 0.1817\n",
      "Epoch: 004/015 | Batch 46080~46591/48000 | Train loss: 0.2089\n",
      "Epoch: 004/015 | Batch 46592~47103/48000 | Train loss: 0.1731\n",
      "Epoch: 004/015 | Batch 47104~47615/48000 | Train loss: 0.1890\n",
      "Epoch: 004/015 | Batch 47616~48127/48000 | Train loss: 0.2270\n",
      "Epoch: 004/015 | Batch 000~511/12000 | Validation loss: 0.2200\n",
      "Epoch: 004/015 | Batch 512~1023/12000 | Validation loss: 0.1937\n",
      "Epoch: 004/015 | Batch 1024~1535/12000 | Validation loss: 0.1803\n",
      "Epoch: 004/015 | Batch 1536~2047/12000 | Validation loss: 0.1975\n",
      "Epoch: 004/015 | Batch 2048~2559/12000 | Validation loss: 0.2515\n",
      "Epoch: 004/015 | Batch 2560~3071/12000 | Validation loss: 0.2133\n",
      "Epoch: 004/015 | Batch 3072~3583/12000 | Validation loss: 0.2293\n",
      "Epoch: 004/015 | Batch 3584~4095/12000 | Validation loss: 0.2426\n",
      "Epoch: 004/015 | Batch 4096~4607/12000 | Validation loss: 0.1681\n",
      "Epoch: 004/015 | Batch 4608~5119/12000 | Validation loss: 0.2430\n",
      "Epoch: 004/015 | Batch 5120~5631/12000 | Validation loss: 0.2841\n",
      "Epoch: 004/015 | Batch 5632~6143/12000 | Validation loss: 0.2111\n",
      "Epoch: 004/015 | Batch 6144~6655/12000 | Validation loss: 0.2108\n",
      "Epoch: 004/015 | Batch 6656~7167/12000 | Validation loss: 0.2474\n",
      "Epoch: 004/015 | Batch 7168~7679/12000 | Validation loss: 0.2293\n",
      "Epoch: 004/015 | Batch 7680~8191/12000 | Validation loss: 0.2051\n",
      "Epoch: 004/015 | Batch 8192~8703/12000 | Validation loss: 0.2176\n",
      "Epoch: 004/015 | Batch 8704~9215/12000 | Validation loss: 0.2477\n",
      "Epoch: 004/015 | Batch 9216~9727/12000 | Validation loss: 0.2062\n",
      "Epoch: 004/015 | Batch 9728~10239/12000 | Validation loss: 0.1902\n",
      "Epoch: 004/015 | Batch 10240~10751/12000 | Validation loss: 0.2276\n",
      "Epoch: 004/015 | Batch 10752~11263/12000 | Validation loss: 0.2177\n",
      "Epoch: 004/015 | Batch 11264~11775/12000 | Validation loss: 0.2145\n",
      "Epoch: 004/015 | Batch 11776~12287/12000 | Validation loss: 0.2302\n",
      "Epoch: 004/015 | Train loss: 0.2323 | Validation loss: 0.2200 \n",
      "Epoch: 004/015 | Train error: 0.0700 | Validation error: 0.0638 \n",
      "Time elapsed: 49.46 min\n",
      "Epoch: 005/015 | Batch 000~511/48000 | Train loss: 0.1881\n",
      "Epoch: 005/015 | Batch 512~1023/48000 | Train loss: 0.1823\n",
      "Epoch: 005/015 | Batch 1024~1535/48000 | Train loss: 0.2617\n",
      "Epoch: 005/015 | Batch 1536~2047/48000 | Train loss: 0.2079\n",
      "Epoch: 005/015 | Batch 2048~2559/48000 | Train loss: 0.2017\n",
      "Epoch: 005/015 | Batch 2560~3071/48000 | Train loss: 0.2344\n",
      "Epoch: 005/015 | Batch 3072~3583/48000 | Train loss: 0.2030\n",
      "Epoch: 005/015 | Batch 3584~4095/48000 | Train loss: 0.2408\n",
      "Epoch: 005/015 | Batch 4096~4607/48000 | Train loss: 0.1750\n",
      "Epoch: 005/015 | Batch 4608~5119/48000 | Train loss: 0.1910\n",
      "Epoch: 005/015 | Batch 5120~5631/48000 | Train loss: 0.2233\n",
      "Epoch: 005/015 | Batch 5632~6143/48000 | Train loss: 0.1828\n",
      "Epoch: 005/015 | Batch 6144~6655/48000 | Train loss: 0.2397\n",
      "Epoch: 005/015 | Batch 6656~7167/48000 | Train loss: 0.2384\n",
      "Epoch: 005/015 | Batch 7168~7679/48000 | Train loss: 0.1994\n",
      "Epoch: 005/015 | Batch 7680~8191/48000 | Train loss: 0.1941\n",
      "Epoch: 005/015 | Batch 8192~8703/48000 | Train loss: 0.2489\n",
      "Epoch: 005/015 | Batch 8704~9215/48000 | Train loss: 0.2100\n",
      "Epoch: 005/015 | Batch 9216~9727/48000 | Train loss: 0.2577\n",
      "Epoch: 005/015 | Batch 9728~10239/48000 | Train loss: 0.1777\n",
      "Epoch: 005/015 | Batch 10240~10751/48000 | Train loss: 0.1895\n",
      "Epoch: 005/015 | Batch 10752~11263/48000 | Train loss: 0.2175\n",
      "Epoch: 005/015 | Batch 11264~11775/48000 | Train loss: 0.2082\n",
      "Epoch: 005/015 | Batch 11776~12287/48000 | Train loss: 0.1345\n",
      "Epoch: 005/015 | Batch 12288~12799/48000 | Train loss: 0.2066\n",
      "Epoch: 005/015 | Batch 12800~13311/48000 | Train loss: 0.1992\n",
      "Epoch: 005/015 | Batch 13312~13823/48000 | Train loss: 0.1706\n",
      "Epoch: 005/015 | Batch 13824~14335/48000 | Train loss: 0.1696\n",
      "Epoch: 005/015 | Batch 14336~14847/48000 | Train loss: 0.1796\n",
      "Epoch: 005/015 | Batch 14848~15359/48000 | Train loss: 0.1957\n",
      "Epoch: 005/015 | Batch 15360~15871/48000 | Train loss: 0.2317\n",
      "Epoch: 005/015 | Batch 15872~16383/48000 | Train loss: 0.1676\n",
      "Epoch: 005/015 | Batch 16384~16895/48000 | Train loss: 0.1771\n",
      "Epoch: 005/015 | Batch 16896~17407/48000 | Train loss: 0.2104\n",
      "Epoch: 005/015 | Batch 17408~17919/48000 | Train loss: 0.1788\n",
      "Epoch: 005/015 | Batch 17920~18431/48000 | Train loss: 0.2084\n",
      "Epoch: 005/015 | Batch 18432~18943/48000 | Train loss: 0.1374\n",
      "Epoch: 005/015 | Batch 18944~19455/48000 | Train loss: 0.2083\n",
      "Epoch: 005/015 | Batch 19456~19967/48000 | Train loss: 0.1925\n",
      "Epoch: 005/015 | Batch 19968~20479/48000 | Train loss: 0.2381\n",
      "Epoch: 005/015 | Batch 20480~20991/48000 | Train loss: 0.1746\n",
      "Epoch: 005/015 | Batch 20992~21503/48000 | Train loss: 0.1877\n",
      "Epoch: 005/015 | Batch 21504~22015/48000 | Train loss: 0.1532\n",
      "Epoch: 005/015 | Batch 22016~22527/48000 | Train loss: 0.1975\n",
      "Epoch: 005/015 | Batch 22528~23039/48000 | Train loss: 0.1693\n",
      "Epoch: 005/015 | Batch 23040~23551/48000 | Train loss: 0.1950\n",
      "Epoch: 005/015 | Batch 23552~24063/48000 | Train loss: 0.1720\n",
      "Epoch: 005/015 | Batch 24064~24575/48000 | Train loss: 0.1545\n",
      "Epoch: 005/015 | Batch 24576~25087/48000 | Train loss: 0.1834\n",
      "Epoch: 005/015 | Batch 25088~25599/48000 | Train loss: 0.2009\n",
      "Epoch: 005/015 | Batch 25600~26111/48000 | Train loss: 0.1716\n",
      "Epoch: 005/015 | Batch 26112~26623/48000 | Train loss: 0.1740\n",
      "Epoch: 005/015 | Batch 26624~27135/48000 | Train loss: 0.1839\n",
      "Epoch: 005/015 | Batch 27136~27647/48000 | Train loss: 0.1760\n",
      "Epoch: 005/015 | Batch 27648~28159/48000 | Train loss: 0.1752\n",
      "Epoch: 005/015 | Batch 28160~28671/48000 | Train loss: 0.1687\n",
      "Epoch: 005/015 | Batch 28672~29183/48000 | Train loss: 0.1568\n",
      "Epoch: 005/015 | Batch 29184~29695/48000 | Train loss: 0.2244\n",
      "Epoch: 005/015 | Batch 29696~30207/48000 | Train loss: 0.2160\n",
      "Epoch: 005/015 | Batch 30208~30719/48000 | Train loss: 0.1750\n",
      "Epoch: 005/015 | Batch 30720~31231/48000 | Train loss: 0.1918\n",
      "Epoch: 005/015 | Batch 31232~31743/48000 | Train loss: 0.1612\n",
      "Epoch: 005/015 | Batch 31744~32255/48000 | Train loss: 0.1734\n",
      "Epoch: 005/015 | Batch 32256~32767/48000 | Train loss: 0.1854\n",
      "Epoch: 005/015 | Batch 32768~33279/48000 | Train loss: 0.1793\n",
      "Epoch: 005/015 | Batch 33280~33791/48000 | Train loss: 0.1985\n",
      "Epoch: 005/015 | Batch 33792~34303/48000 | Train loss: 0.2304\n",
      "Epoch: 005/015 | Batch 34304~34815/48000 | Train loss: 0.1485\n",
      "Epoch: 005/015 | Batch 34816~35327/48000 | Train loss: 0.2058\n",
      "Epoch: 005/015 | Batch 35328~35839/48000 | Train loss: 0.1432\n",
      "Epoch: 005/015 | Batch 35840~36351/48000 | Train loss: 0.1774\n",
      "Epoch: 005/015 | Batch 36352~36863/48000 | Train loss: 0.1657\n",
      "Epoch: 005/015 | Batch 36864~37375/48000 | Train loss: 0.1921\n",
      "Epoch: 005/015 | Batch 37376~37887/48000 | Train loss: 0.1772\n",
      "Epoch: 005/015 | Batch 37888~38399/48000 | Train loss: 0.2160\n",
      "Epoch: 005/015 | Batch 38400~38911/48000 | Train loss: 0.2043\n",
      "Epoch: 005/015 | Batch 38912~39423/48000 | Train loss: 0.1990\n",
      "Epoch: 005/015 | Batch 39424~39935/48000 | Train loss: 0.2279\n",
      "Epoch: 005/015 | Batch 39936~40447/48000 | Train loss: 0.1931\n",
      "Epoch: 005/015 | Batch 40448~40959/48000 | Train loss: 0.1913\n",
      "Epoch: 005/015 | Batch 40960~41471/48000 | Train loss: 0.1534\n",
      "Epoch: 005/015 | Batch 41472~41983/48000 | Train loss: 0.1723\n",
      "Epoch: 005/015 | Batch 41984~42495/48000 | Train loss: 0.1553\n",
      "Epoch: 005/015 | Batch 42496~43007/48000 | Train loss: 0.1936\n",
      "Epoch: 005/015 | Batch 43008~43519/48000 | Train loss: 0.1604\n",
      "Epoch: 005/015 | Batch 43520~44031/48000 | Train loss: 0.1263\n",
      "Epoch: 005/015 | Batch 44032~44543/48000 | Train loss: 0.1575\n",
      "Epoch: 005/015 | Batch 44544~45055/48000 | Train loss: 0.1744\n",
      "Epoch: 005/015 | Batch 45056~45567/48000 | Train loss: 0.1783\n",
      "Epoch: 005/015 | Batch 45568~46079/48000 | Train loss: 0.1717\n",
      "Epoch: 005/015 | Batch 46080~46591/48000 | Train loss: 0.1819\n",
      "Epoch: 005/015 | Batch 46592~47103/48000 | Train loss: 0.1613\n",
      "Epoch: 005/015 | Batch 47104~47615/48000 | Train loss: 0.2051\n",
      "Epoch: 005/015 | Batch 47616~48127/48000 | Train loss: 0.2119\n",
      "Epoch: 005/015 | Batch 000~511/12000 | Validation loss: 0.1658\n",
      "Epoch: 005/015 | Batch 512~1023/12000 | Validation loss: 0.1333\n",
      "Epoch: 005/015 | Batch 1024~1535/12000 | Validation loss: 0.1329\n",
      "Epoch: 005/015 | Batch 1536~2047/12000 | Validation loss: 0.1605\n",
      "Epoch: 005/015 | Batch 2048~2559/12000 | Validation loss: 0.1686\n",
      "Epoch: 005/015 | Batch 2560~3071/12000 | Validation loss: 0.1639\n",
      "Epoch: 005/015 | Batch 3072~3583/12000 | Validation loss: 0.1868\n",
      "Epoch: 005/015 | Batch 3584~4095/12000 | Validation loss: 0.1702\n",
      "Epoch: 005/015 | Batch 4096~4607/12000 | Validation loss: 0.1285\n",
      "Epoch: 005/015 | Batch 4608~5119/12000 | Validation loss: 0.1820\n",
      "Epoch: 005/015 | Batch 5120~5631/12000 | Validation loss: 0.2408\n",
      "Epoch: 005/015 | Batch 5632~6143/12000 | Validation loss: 0.1674\n",
      "Epoch: 005/015 | Batch 6144~6655/12000 | Validation loss: 0.1813\n",
      "Epoch: 005/015 | Batch 6656~7167/12000 | Validation loss: 0.1953\n",
      "Epoch: 005/015 | Batch 7168~7679/12000 | Validation loss: 0.1937\n",
      "Epoch: 005/015 | Batch 7680~8191/12000 | Validation loss: 0.1379\n",
      "Epoch: 005/015 | Batch 8192~8703/12000 | Validation loss: 0.1668\n",
      "Epoch: 005/015 | Batch 8704~9215/12000 | Validation loss: 0.1765\n",
      "Epoch: 005/015 | Batch 9216~9727/12000 | Validation loss: 0.1546\n",
      "Epoch: 005/015 | Batch 9728~10239/12000 | Validation loss: 0.1424\n",
      "Epoch: 005/015 | Batch 10240~10751/12000 | Validation loss: 0.1710\n",
      "Epoch: 005/015 | Batch 10752~11263/12000 | Validation loss: 0.1849\n",
      "Epoch: 005/015 | Batch 11264~11775/12000 | Validation loss: 0.1610\n",
      "Epoch: 005/015 | Batch 11776~12287/12000 | Validation loss: 0.2077\n",
      "Epoch: 005/015 | Train loss: 0.1899 | Validation loss: 0.1697 \n",
      "Epoch: 005/015 | Train error: 0.0575 | Validation error: 0.0496 \n",
      "Time elapsed: 49.23 min\n",
      "Epoch: 006/015 | Batch 000~511/48000 | Train loss: 0.1486\n",
      "Epoch: 006/015 | Batch 512~1023/48000 | Train loss: 0.1535\n",
      "Epoch: 006/015 | Batch 1024~1535/48000 | Train loss: 0.1695\n",
      "Epoch: 006/015 | Batch 1536~2047/48000 | Train loss: 0.1763\n",
      "Epoch: 006/015 | Batch 2048~2559/48000 | Train loss: 0.2105\n",
      "Epoch: 006/015 | Batch 2560~3071/48000 | Train loss: 0.1430\n",
      "Epoch: 006/015 | Batch 3072~3583/48000 | Train loss: 0.1753\n",
      "Epoch: 006/015 | Batch 3584~4095/48000 | Train loss: 0.1851\n",
      "Epoch: 006/015 | Batch 4096~4607/48000 | Train loss: 0.2084\n",
      "Epoch: 006/015 | Batch 4608~5119/48000 | Train loss: 0.1444\n",
      "Epoch: 006/015 | Batch 5120~5631/48000 | Train loss: 0.1781\n",
      "Epoch: 006/015 | Batch 5632~6143/48000 | Train loss: 0.1477\n",
      "Epoch: 006/015 | Batch 6144~6655/48000 | Train loss: 0.1334\n",
      "Epoch: 006/015 | Batch 6656~7167/48000 | Train loss: 0.1782\n",
      "Epoch: 006/015 | Batch 7168~7679/48000 | Train loss: 0.1535\n",
      "Epoch: 006/015 | Batch 7680~8191/48000 | Train loss: 0.1833\n",
      "Epoch: 006/015 | Batch 8192~8703/48000 | Train loss: 0.1511\n",
      "Epoch: 006/015 | Batch 8704~9215/48000 | Train loss: 0.1821\n",
      "Epoch: 006/015 | Batch 9216~9727/48000 | Train loss: 0.1704\n",
      "Epoch: 006/015 | Batch 9728~10239/48000 | Train loss: 0.1330\n",
      "Epoch: 006/015 | Batch 10240~10751/48000 | Train loss: 0.1859\n",
      "Epoch: 006/015 | Batch 10752~11263/48000 | Train loss: 0.2181\n",
      "Epoch: 006/015 | Batch 11264~11775/48000 | Train loss: 0.1576\n",
      "Epoch: 006/015 | Batch 11776~12287/48000 | Train loss: 0.1624\n",
      "Epoch: 006/015 | Batch 12288~12799/48000 | Train loss: 0.1944\n",
      "Epoch: 006/015 | Batch 12800~13311/48000 | Train loss: 0.1618\n",
      "Epoch: 006/015 | Batch 13312~13823/48000 | Train loss: 0.1814\n",
      "Epoch: 006/015 | Batch 13824~14335/48000 | Train loss: 0.2118\n",
      "Epoch: 006/015 | Batch 14336~14847/48000 | Train loss: 0.1676\n",
      "Epoch: 006/015 | Batch 14848~15359/48000 | Train loss: 0.1740\n",
      "Epoch: 006/015 | Batch 15360~15871/48000 | Train loss: 0.1597\n",
      "Epoch: 006/015 | Batch 15872~16383/48000 | Train loss: 0.1732\n",
      "Epoch: 006/015 | Batch 16384~16895/48000 | Train loss: 0.1390\n",
      "Epoch: 006/015 | Batch 16896~17407/48000 | Train loss: 0.1689\n",
      "Epoch: 006/015 | Batch 17408~17919/48000 | Train loss: 0.1610\n",
      "Epoch: 006/015 | Batch 17920~18431/48000 | Train loss: 0.2280\n",
      "Epoch: 006/015 | Batch 18432~18943/48000 | Train loss: 0.1054\n",
      "Epoch: 006/015 | Batch 18944~19455/48000 | Train loss: 0.1896\n",
      "Epoch: 006/015 | Batch 19456~19967/48000 | Train loss: 0.1291\n",
      "Epoch: 006/015 | Batch 19968~20479/48000 | Train loss: 0.1657\n",
      "Epoch: 006/015 | Batch 20480~20991/48000 | Train loss: 0.1813\n",
      "Epoch: 006/015 | Batch 20992~21503/48000 | Train loss: 0.1309\n",
      "Epoch: 006/015 | Batch 21504~22015/48000 | Train loss: 0.1799\n",
      "Epoch: 006/015 | Batch 22016~22527/48000 | Train loss: 0.1264\n",
      "Epoch: 006/015 | Batch 22528~23039/48000 | Train loss: 0.1409\n",
      "Epoch: 006/015 | Batch 23040~23551/48000 | Train loss: 0.1352\n",
      "Epoch: 006/015 | Batch 23552~24063/48000 | Train loss: 0.1634\n",
      "Epoch: 006/015 | Batch 24064~24575/48000 | Train loss: 0.1939\n",
      "Epoch: 006/015 | Batch 24576~25087/48000 | Train loss: 0.1610\n",
      "Epoch: 006/015 | Batch 25088~25599/48000 | Train loss: 0.1290\n",
      "Epoch: 006/015 | Batch 25600~26111/48000 | Train loss: 0.1832\n",
      "Epoch: 006/015 | Batch 26112~26623/48000 | Train loss: 0.1682\n",
      "Epoch: 006/015 | Batch 26624~27135/48000 | Train loss: 0.1672\n",
      "Epoch: 006/015 | Batch 27136~27647/48000 | Train loss: 0.1516\n",
      "Epoch: 006/015 | Batch 27648~28159/48000 | Train loss: 0.1707\n",
      "Epoch: 006/015 | Batch 28160~28671/48000 | Train loss: 0.1997\n",
      "Epoch: 006/015 | Batch 28672~29183/48000 | Train loss: 0.1554\n",
      "Epoch: 006/015 | Batch 29184~29695/48000 | Train loss: 0.1627\n",
      "Epoch: 006/015 | Batch 29696~30207/48000 | Train loss: 0.1631\n",
      "Epoch: 006/015 | Batch 30208~30719/48000 | Train loss: 0.1874\n",
      "Epoch: 006/015 | Batch 30720~31231/48000 | Train loss: 0.1817\n",
      "Epoch: 006/015 | Batch 31232~31743/48000 | Train loss: 0.1617\n",
      "Epoch: 006/015 | Batch 31744~32255/48000 | Train loss: 0.1496\n",
      "Epoch: 006/015 | Batch 32256~32767/48000 | Train loss: 0.1331\n",
      "Epoch: 006/015 | Batch 32768~33279/48000 | Train loss: 0.1780\n",
      "Epoch: 006/015 | Batch 33280~33791/48000 | Train loss: 0.1475\n",
      "Epoch: 006/015 | Batch 33792~34303/48000 | Train loss: 0.1698\n",
      "Epoch: 006/015 | Batch 34304~34815/48000 | Train loss: 0.1075\n",
      "Epoch: 006/015 | Batch 34816~35327/48000 | Train loss: 0.1652\n",
      "Epoch: 006/015 | Batch 35328~35839/48000 | Train loss: 0.1220\n",
      "Epoch: 006/015 | Batch 35840~36351/48000 | Train loss: 0.1634\n",
      "Epoch: 006/015 | Batch 36352~36863/48000 | Train loss: 0.1549\n",
      "Epoch: 006/015 | Batch 36864~37375/48000 | Train loss: 0.1457\n",
      "Epoch: 006/015 | Batch 37376~37887/48000 | Train loss: 0.1631\n",
      "Epoch: 006/015 | Batch 37888~38399/48000 | Train loss: 0.1355\n",
      "Epoch: 006/015 | Batch 38400~38911/48000 | Train loss: 0.1485\n",
      "Epoch: 006/015 | Batch 38912~39423/48000 | Train loss: 0.1797\n",
      "Epoch: 006/015 | Batch 39424~39935/48000 | Train loss: 0.1598\n",
      "Epoch: 006/015 | Batch 39936~40447/48000 | Train loss: 0.1945\n",
      "Epoch: 006/015 | Batch 40448~40959/48000 | Train loss: 0.1738\n",
      "Epoch: 006/015 | Batch 40960~41471/48000 | Train loss: 0.1654\n",
      "Epoch: 006/015 | Batch 41472~41983/48000 | Train loss: 0.1797\n",
      "Epoch: 006/015 | Batch 41984~42495/48000 | Train loss: 0.1739\n",
      "Epoch: 006/015 | Batch 42496~43007/48000 | Train loss: 0.1590\n",
      "Epoch: 006/015 | Batch 43008~43519/48000 | Train loss: 0.1539\n",
      "Epoch: 006/015 | Batch 43520~44031/48000 | Train loss: 0.1606\n",
      "Epoch: 006/015 | Batch 44032~44543/48000 | Train loss: 0.1049\n",
      "Epoch: 006/015 | Batch 44544~45055/48000 | Train loss: 0.1680\n",
      "Epoch: 006/015 | Batch 45056~45567/48000 | Train loss: 0.1556\n",
      "Epoch: 006/015 | Batch 45568~46079/48000 | Train loss: 0.1939\n",
      "Epoch: 006/015 | Batch 46080~46591/48000 | Train loss: 0.1740\n",
      "Epoch: 006/015 | Batch 46592~47103/48000 | Train loss: 0.2084\n",
      "Epoch: 006/015 | Batch 47104~47615/48000 | Train loss: 0.1452\n",
      "Epoch: 006/015 | Batch 47616~48127/48000 | Train loss: 0.1970\n",
      "Epoch: 006/015 | Batch 000~511/12000 | Validation loss: 0.1562\n",
      "Epoch: 006/015 | Batch 512~1023/12000 | Validation loss: 0.1366\n",
      "Epoch: 006/015 | Batch 1024~1535/12000 | Validation loss: 0.1349\n",
      "Epoch: 006/015 | Batch 1536~2047/12000 | Validation loss: 0.1526\n",
      "Epoch: 006/015 | Batch 2048~2559/12000 | Validation loss: 0.1713\n",
      "Epoch: 006/015 | Batch 2560~3071/12000 | Validation loss: 0.1505\n",
      "Epoch: 006/015 | Batch 3072~3583/12000 | Validation loss: 0.1818\n",
      "Epoch: 006/015 | Batch 3584~4095/12000 | Validation loss: 0.1591\n",
      "Epoch: 006/015 | Batch 4096~4607/12000 | Validation loss: 0.1328\n",
      "Epoch: 006/015 | Batch 4608~5119/12000 | Validation loss: 0.1763\n",
      "Epoch: 006/015 | Batch 5120~5631/12000 | Validation loss: 0.2364\n",
      "Epoch: 006/015 | Batch 5632~6143/12000 | Validation loss: 0.1883\n",
      "Epoch: 006/015 | Batch 6144~6655/12000 | Validation loss: 0.1861\n",
      "Epoch: 006/015 | Batch 6656~7167/12000 | Validation loss: 0.2104\n",
      "Epoch: 006/015 | Batch 7168~7679/12000 | Validation loss: 0.1715\n",
      "Epoch: 006/015 | Batch 7680~8191/12000 | Validation loss: 0.1396\n",
      "Epoch: 006/015 | Batch 8192~8703/12000 | Validation loss: 0.1544\n",
      "Epoch: 006/015 | Batch 8704~9215/12000 | Validation loss: 0.1708\n",
      "Epoch: 006/015 | Batch 9216~9727/12000 | Validation loss: 0.1519\n",
      "Epoch: 006/015 | Batch 9728~10239/12000 | Validation loss: 0.1296\n",
      "Epoch: 006/015 | Batch 10240~10751/12000 | Validation loss: 0.1622\n",
      "Epoch: 006/015 | Batch 10752~11263/12000 | Validation loss: 0.1798\n",
      "Epoch: 006/015 | Batch 11264~11775/12000 | Validation loss: 0.1612\n",
      "Epoch: 006/015 | Batch 11776~12287/12000 | Validation loss: 0.1723\n",
      "Epoch: 006/015 | Train loss: 0.1647 | Validation loss: 0.1653 \n",
      "Epoch: 006/015 | Train error: 0.0503 | Validation error: 0.0506 \n",
      "Time elapsed: 49.15 min\n",
      "Epoch: 007/015 | Batch 000~511/48000 | Train loss: 0.1736\n",
      "Epoch: 007/015 | Batch 512~1023/48000 | Train loss: 0.1677\n",
      "Epoch: 007/015 | Batch 1024~1535/48000 | Train loss: 0.1641\n",
      "Epoch: 007/015 | Batch 1536~2047/48000 | Train loss: 0.1551\n",
      "Epoch: 007/015 | Batch 2048~2559/48000 | Train loss: 0.1615\n",
      "Epoch: 007/015 | Batch 2560~3071/48000 | Train loss: 0.1952\n",
      "Epoch: 007/015 | Batch 3072~3583/48000 | Train loss: 0.1688\n",
      "Epoch: 007/015 | Batch 3584~4095/48000 | Train loss: 0.0960\n",
      "Epoch: 007/015 | Batch 4096~4607/48000 | Train loss: 0.1471\n",
      "Epoch: 007/015 | Batch 4608~5119/48000 | Train loss: 0.1171\n",
      "Epoch: 007/015 | Batch 5120~5631/48000 | Train loss: 0.1427\n",
      "Epoch: 007/015 | Batch 5632~6143/48000 | Train loss: 0.1233\n",
      "Epoch: 007/015 | Batch 6144~6655/48000 | Train loss: 0.0955\n",
      "Epoch: 007/015 | Batch 6656~7167/48000 | Train loss: 0.1288\n",
      "Epoch: 007/015 | Batch 7168~7679/48000 | Train loss: 0.1592\n",
      "Epoch: 007/015 | Batch 7680~8191/48000 | Train loss: 0.1580\n",
      "Epoch: 007/015 | Batch 8192~8703/48000 | Train loss: 0.1909\n",
      "Epoch: 007/015 | Batch 8704~9215/48000 | Train loss: 0.1915\n",
      "Epoch: 007/015 | Batch 9216~9727/48000 | Train loss: 0.1580\n",
      "Epoch: 007/015 | Batch 9728~10239/48000 | Train loss: 0.1170\n",
      "Epoch: 007/015 | Batch 10240~10751/48000 | Train loss: 0.1279\n",
      "Epoch: 007/015 | Batch 10752~11263/48000 | Train loss: 0.1791\n",
      "Epoch: 007/015 | Batch 11264~11775/48000 | Train loss: 0.1198\n",
      "Epoch: 007/015 | Batch 11776~12287/48000 | Train loss: 0.1643\n",
      "Epoch: 007/015 | Batch 12288~12799/48000 | Train loss: 0.2103\n",
      "Epoch: 007/015 | Batch 12800~13311/48000 | Train loss: 0.1611\n",
      "Epoch: 007/015 | Batch 13312~13823/48000 | Train loss: 0.1572\n",
      "Epoch: 007/015 | Batch 13824~14335/48000 | Train loss: 0.1652\n",
      "Epoch: 007/015 | Batch 14336~14847/48000 | Train loss: 0.1516\n",
      "Epoch: 007/015 | Batch 14848~15359/48000 | Train loss: 0.2034\n",
      "Epoch: 007/015 | Batch 15360~15871/48000 | Train loss: 0.2029\n",
      "Epoch: 007/015 | Batch 15872~16383/48000 | Train loss: 0.1530\n",
      "Epoch: 007/015 | Batch 16384~16895/48000 | Train loss: 0.1338\n",
      "Epoch: 007/015 | Batch 16896~17407/48000 | Train loss: 0.1865\n",
      "Epoch: 007/015 | Batch 17408~17919/48000 | Train loss: 0.1760\n",
      "Epoch: 007/015 | Batch 17920~18431/48000 | Train loss: 0.1664\n",
      "Epoch: 007/015 | Batch 18432~18943/48000 | Train loss: 0.1448\n",
      "Epoch: 007/015 | Batch 18944~19455/48000 | Train loss: 0.1457\n",
      "Epoch: 007/015 | Batch 19456~19967/48000 | Train loss: 0.1848\n",
      "Epoch: 007/015 | Batch 19968~20479/48000 | Train loss: 0.1244\n",
      "Epoch: 007/015 | Batch 20480~20991/48000 | Train loss: 0.1489\n",
      "Epoch: 007/015 | Batch 20992~21503/48000 | Train loss: 0.1229\n",
      "Epoch: 007/015 | Batch 21504~22015/48000 | Train loss: 0.1446\n",
      "Epoch: 007/015 | Batch 22016~22527/48000 | Train loss: 0.1272\n",
      "Epoch: 007/015 | Batch 22528~23039/48000 | Train loss: 0.1619\n",
      "Epoch: 007/015 | Batch 23040~23551/48000 | Train loss: 0.1507\n",
      "Epoch: 007/015 | Batch 23552~24063/48000 | Train loss: 0.1304\n",
      "Epoch: 007/015 | Batch 24064~24575/48000 | Train loss: 0.1564\n",
      "Epoch: 007/015 | Batch 24576~25087/48000 | Train loss: 0.1437\n",
      "Epoch: 007/015 | Batch 25088~25599/48000 | Train loss: 0.1378\n",
      "Epoch: 007/015 | Batch 25600~26111/48000 | Train loss: 0.1551\n",
      "Epoch: 007/015 | Batch 26112~26623/48000 | Train loss: 0.1792\n",
      "Epoch: 007/015 | Batch 26624~27135/48000 | Train loss: 0.1806\n",
      "Epoch: 007/015 | Batch 27136~27647/48000 | Train loss: 0.1223\n",
      "Epoch: 007/015 | Batch 27648~28159/48000 | Train loss: 0.1597\n",
      "Epoch: 007/015 | Batch 28160~28671/48000 | Train loss: 0.1267\n",
      "Epoch: 007/015 | Batch 28672~29183/48000 | Train loss: 0.1587\n",
      "Epoch: 007/015 | Batch 29184~29695/48000 | Train loss: 0.2071\n",
      "Epoch: 007/015 | Batch 29696~30207/48000 | Train loss: 0.2246\n",
      "Epoch: 007/015 | Batch 30208~30719/48000 | Train loss: 0.1806\n",
      "Epoch: 007/015 | Batch 30720~31231/48000 | Train loss: 0.1613\n",
      "Epoch: 007/015 | Batch 31232~31743/48000 | Train loss: 0.1602\n",
      "Epoch: 007/015 | Batch 31744~32255/48000 | Train loss: 0.1600\n",
      "Epoch: 007/015 | Batch 32256~32767/48000 | Train loss: 0.1143\n",
      "Epoch: 007/015 | Batch 32768~33279/48000 | Train loss: 0.1827\n",
      "Epoch: 007/015 | Batch 33280~33791/48000 | Train loss: 0.1996\n",
      "Epoch: 007/015 | Batch 33792~34303/48000 | Train loss: 0.1353\n",
      "Epoch: 007/015 | Batch 34304~34815/48000 | Train loss: 0.1752\n",
      "Epoch: 007/015 | Batch 34816~35327/48000 | Train loss: 0.1591\n",
      "Epoch: 007/015 | Batch 35328~35839/48000 | Train loss: 0.1621\n",
      "Epoch: 007/015 | Batch 35840~36351/48000 | Train loss: 0.1227\n",
      "Epoch: 007/015 | Batch 36352~36863/48000 | Train loss: 0.3288\n",
      "Epoch: 007/015 | Batch 36864~37375/48000 | Train loss: 0.9618\n",
      "Epoch: 007/015 | Batch 37376~37887/48000 | Train loss: 0.5879\n",
      "Epoch: 007/015 | Batch 37888~38399/48000 | Train loss: 0.2662\n",
      "Epoch: 007/015 | Batch 38400~38911/48000 | Train loss: 0.2691\n",
      "Epoch: 007/015 | Batch 38912~39423/48000 | Train loss: 0.2253\n",
      "Epoch: 007/015 | Batch 39424~39935/48000 | Train loss: 0.2879\n",
      "Epoch: 007/015 | Batch 39936~40447/48000 | Train loss: 0.2050\n",
      "Epoch: 007/015 | Batch 40448~40959/48000 | Train loss: 0.2291\n",
      "Epoch: 007/015 | Batch 40960~41471/48000 | Train loss: 0.1850\n",
      "Epoch: 007/015 | Batch 41472~41983/48000 | Train loss: 0.2860\n",
      "Epoch: 007/015 | Batch 41984~42495/48000 | Train loss: 0.1835\n",
      "Epoch: 007/015 | Batch 42496~43007/48000 | Train loss: 0.2301\n",
      "Epoch: 007/015 | Batch 43008~43519/48000 | Train loss: 0.2334\n",
      "Epoch: 007/015 | Batch 43520~44031/48000 | Train loss: 0.2074\n",
      "Epoch: 007/015 | Batch 44032~44543/48000 | Train loss: 0.2421\n",
      "Epoch: 007/015 | Batch 44544~45055/48000 | Train loss: 0.1944\n",
      "Epoch: 007/015 | Batch 45056~45567/48000 | Train loss: 0.2294\n",
      "Epoch: 007/015 | Batch 45568~46079/48000 | Train loss: 0.1924\n",
      "Epoch: 007/015 | Batch 46080~46591/48000 | Train loss: 0.1891\n",
      "Epoch: 007/015 | Batch 46592~47103/48000 | Train loss: 0.1807\n",
      "Epoch: 007/015 | Batch 47104~47615/48000 | Train loss: 0.2197\n",
      "Epoch: 007/015 | Batch 47616~48127/48000 | Train loss: 0.1636\n",
      "Epoch: 007/015 | Batch 000~511/12000 | Validation loss: 0.1905\n",
      "Epoch: 007/015 | Batch 512~1023/12000 | Validation loss: 0.1580\n",
      "Epoch: 007/015 | Batch 1024~1535/12000 | Validation loss: 0.1380\n",
      "Epoch: 007/015 | Batch 1536~2047/12000 | Validation loss: 0.1801\n",
      "Epoch: 007/015 | Batch 2048~2559/12000 | Validation loss: 0.2239\n",
      "Epoch: 007/015 | Batch 2560~3071/12000 | Validation loss: 0.1619\n",
      "Epoch: 007/015 | Batch 3072~3583/12000 | Validation loss: 0.2184\n",
      "Epoch: 007/015 | Batch 3584~4095/12000 | Validation loss: 0.1934\n",
      "Epoch: 007/015 | Batch 4096~4607/12000 | Validation loss: 0.1398\n",
      "Epoch: 007/015 | Batch 4608~5119/12000 | Validation loss: 0.1962\n",
      "Epoch: 007/015 | Batch 5120~5631/12000 | Validation loss: 0.2475\n",
      "Epoch: 007/015 | Batch 5632~6143/12000 | Validation loss: 0.2040\n",
      "Epoch: 007/015 | Batch 6144~6655/12000 | Validation loss: 0.2247\n",
      "Epoch: 007/015 | Batch 6656~7167/12000 | Validation loss: 0.2264\n",
      "Epoch: 007/015 | Batch 7168~7679/12000 | Validation loss: 0.2245\n",
      "Epoch: 007/015 | Batch 7680~8191/12000 | Validation loss: 0.1499\n",
      "Epoch: 007/015 | Batch 8192~8703/12000 | Validation loss: 0.1846\n",
      "Epoch: 007/015 | Batch 8704~9215/12000 | Validation loss: 0.2261\n",
      "Epoch: 007/015 | Batch 9216~9727/12000 | Validation loss: 0.1753\n",
      "Epoch: 007/015 | Batch 9728~10239/12000 | Validation loss: 0.1531\n",
      "Epoch: 007/015 | Batch 10240~10751/12000 | Validation loss: 0.2166\n",
      "Epoch: 007/015 | Batch 10752~11263/12000 | Validation loss: 0.2016\n",
      "Epoch: 007/015 | Batch 11264~11775/12000 | Validation loss: 0.1913\n",
      "Epoch: 007/015 | Batch 11776~12287/12000 | Validation loss: 0.1929\n",
      "Epoch: 007/015 | Train loss: 0.1853 | Validation loss: 0.1924 \n",
      "Epoch: 007/015 | Train error: 0.0560 | Validation error: 0.0580 \n",
      "Time elapsed: 49.17 min\n",
      "Epoch: 008/015 | Batch 000~511/48000 | Train loss: 0.1278\n",
      "Epoch: 008/015 | Batch 512~1023/48000 | Train loss: 0.1999\n",
      "Epoch: 008/015 | Batch 1024~1535/48000 | Train loss: 0.2410\n",
      "Epoch: 008/015 | Batch 1536~2047/48000 | Train loss: 0.1967\n",
      "Epoch: 008/015 | Batch 2048~2559/48000 | Train loss: 0.1574\n",
      "Epoch: 008/015 | Batch 2560~3071/48000 | Train loss: 0.2193\n",
      "Epoch: 008/015 | Batch 3072~3583/48000 | Train loss: 0.1945\n",
      "Epoch: 008/015 | Batch 3584~4095/48000 | Train loss: 0.2056\n",
      "Epoch: 008/015 | Batch 4096~4607/48000 | Train loss: 0.1675\n",
      "Epoch: 008/015 | Batch 4608~5119/48000 | Train loss: 0.1526\n",
      "Epoch: 008/015 | Batch 5120~5631/48000 | Train loss: 0.1722\n",
      "Epoch: 008/015 | Batch 5632~6143/48000 | Train loss: 0.1679\n",
      "Epoch: 008/015 | Batch 6144~6655/48000 | Train loss: 0.1813\n",
      "Epoch: 008/015 | Batch 6656~7167/48000 | Train loss: 0.1924\n",
      "Epoch: 008/015 | Batch 7168~7679/48000 | Train loss: 0.1724\n",
      "Epoch: 008/015 | Batch 7680~8191/48000 | Train loss: 0.1861\n",
      "Epoch: 008/015 | Batch 8192~8703/48000 | Train loss: 0.1580\n",
      "Epoch: 008/015 | Batch 8704~9215/48000 | Train loss: 0.2269\n",
      "Epoch: 008/015 | Batch 9216~9727/48000 | Train loss: 0.1564\n",
      "Epoch: 008/015 | Batch 9728~10239/48000 | Train loss: 0.1927\n",
      "Epoch: 008/015 | Batch 10240~10751/48000 | Train loss: 0.2482\n",
      "Epoch: 008/015 | Batch 10752~11263/48000 | Train loss: 0.2434\n",
      "Epoch: 008/015 | Batch 11264~11775/48000 | Train loss: 0.1745\n",
      "Epoch: 008/015 | Batch 11776~12287/48000 | Train loss: 0.1766\n",
      "Epoch: 008/015 | Batch 12288~12799/48000 | Train loss: 0.1672\n",
      "Epoch: 008/015 | Batch 12800~13311/48000 | Train loss: 0.1337\n",
      "Epoch: 008/015 | Batch 13312~13823/48000 | Train loss: 0.1521\n",
      "Epoch: 008/015 | Batch 13824~14335/48000 | Train loss: 0.1693\n",
      "Epoch: 008/015 | Batch 14336~14847/48000 | Train loss: 0.1831\n",
      "Epoch: 008/015 | Batch 14848~15359/48000 | Train loss: 0.1924\n",
      "Epoch: 008/015 | Batch 15360~15871/48000 | Train loss: 0.1796\n",
      "Epoch: 008/015 | Batch 15872~16383/48000 | Train loss: 0.1586\n",
      "Epoch: 008/015 | Batch 16384~16895/48000 | Train loss: 0.1671\n",
      "Epoch: 008/015 | Batch 16896~17407/48000 | Train loss: 0.1256\n",
      "Epoch: 008/015 | Batch 17408~17919/48000 | Train loss: 0.1596\n",
      "Epoch: 008/015 | Batch 17920~18431/48000 | Train loss: 0.1906\n",
      "Epoch: 008/015 | Batch 18432~18943/48000 | Train loss: 0.1595\n",
      "Epoch: 008/015 | Batch 18944~19455/48000 | Train loss: 0.1626\n",
      "Epoch: 008/015 | Batch 19456~19967/48000 | Train loss: 0.1660\n",
      "Epoch: 008/015 | Batch 19968~20479/48000 | Train loss: 0.1692\n",
      "Epoch: 008/015 | Batch 20480~20991/48000 | Train loss: 0.1778\n",
      "Epoch: 008/015 | Batch 20992~21503/48000 | Train loss: 0.1399\n",
      "Epoch: 008/015 | Batch 21504~22015/48000 | Train loss: 0.2469\n",
      "Epoch: 008/015 | Batch 22016~22527/48000 | Train loss: 0.2019\n",
      "Epoch: 008/015 | Batch 22528~23039/48000 | Train loss: 0.1799\n",
      "Epoch: 008/015 | Batch 23040~23551/48000 | Train loss: 0.1614\n",
      "Epoch: 008/015 | Batch 23552~24063/48000 | Train loss: 0.1369\n",
      "Epoch: 008/015 | Batch 24064~24575/48000 | Train loss: 0.1523\n",
      "Epoch: 008/015 | Batch 24576~25087/48000 | Train loss: 0.1368\n",
      "Epoch: 008/015 | Batch 25088~25599/48000 | Train loss: 0.1707\n",
      "Epoch: 008/015 | Batch 25600~26111/48000 | Train loss: 0.1845\n",
      "Epoch: 008/015 | Batch 26112~26623/48000 | Train loss: 0.1374\n",
      "Epoch: 008/015 | Batch 26624~27135/48000 | Train loss: 0.1616\n",
      "Epoch: 008/015 | Batch 27136~27647/48000 | Train loss: 0.1535\n",
      "Epoch: 008/015 | Batch 27648~28159/48000 | Train loss: 0.1790\n",
      "Epoch: 008/015 | Batch 28160~28671/48000 | Train loss: 0.1864\n",
      "Epoch: 008/015 | Batch 28672~29183/48000 | Train loss: 0.1930\n",
      "Epoch: 008/015 | Batch 29184~29695/48000 | Train loss: 0.1681\n",
      "Epoch: 008/015 | Batch 29696~30207/48000 | Train loss: 0.1413\n",
      "Epoch: 008/015 | Batch 30208~30719/48000 | Train loss: 0.1703\n",
      "Epoch: 008/015 | Batch 30720~31231/48000 | Train loss: 0.1297\n",
      "Epoch: 008/015 | Batch 31232~31743/48000 | Train loss: 0.1363\n",
      "Epoch: 008/015 | Batch 31744~32255/48000 | Train loss: 0.1303\n",
      "Epoch: 008/015 | Batch 32256~32767/48000 | Train loss: 0.1220\n",
      "Epoch: 008/015 | Batch 32768~33279/48000 | Train loss: 0.1835\n",
      "Epoch: 008/015 | Batch 33280~33791/48000 | Train loss: 0.2231\n",
      "Epoch: 008/015 | Batch 33792~34303/48000 | Train loss: 0.1320\n",
      "Epoch: 008/015 | Batch 34304~34815/48000 | Train loss: 0.1599\n",
      "Epoch: 008/015 | Batch 34816~35327/48000 | Train loss: 0.1518\n",
      "Epoch: 008/015 | Batch 35328~35839/48000 | Train loss: 0.1446\n",
      "Epoch: 008/015 | Batch 35840~36351/48000 | Train loss: 0.1689\n",
      "Epoch: 008/015 | Batch 36352~36863/48000 | Train loss: 0.1803\n",
      "Epoch: 008/015 | Batch 36864~37375/48000 | Train loss: 0.1686\n",
      "Epoch: 008/015 | Batch 37376~37887/48000 | Train loss: 0.1374\n",
      "Epoch: 008/015 | Batch 37888~38399/48000 | Train loss: 0.1369\n",
      "Epoch: 008/015 | Batch 38400~38911/48000 | Train loss: 0.1582\n",
      "Epoch: 008/015 | Batch 38912~39423/48000 | Train loss: 0.1634\n",
      "Epoch: 008/015 | Batch 39424~39935/48000 | Train loss: 0.1595\n",
      "Epoch: 008/015 | Batch 39936~40447/48000 | Train loss: 0.1881\n",
      "Epoch: 008/015 | Batch 40448~40959/48000 | Train loss: 0.1739\n",
      "Epoch: 008/015 | Batch 40960~41471/48000 | Train loss: 0.1404\n",
      "Epoch: 008/015 | Batch 41472~41983/48000 | Train loss: 0.1749\n",
      "Epoch: 008/015 | Batch 41984~42495/48000 | Train loss: 0.1679\n",
      "Epoch: 008/015 | Batch 42496~43007/48000 | Train loss: 0.1785\n",
      "Epoch: 008/015 | Batch 43008~43519/48000 | Train loss: 0.1364\n",
      "Epoch: 008/015 | Batch 43520~44031/48000 | Train loss: 0.1714\n",
      "Epoch: 008/015 | Batch 44032~44543/48000 | Train loss: 0.1619\n",
      "Epoch: 008/015 | Batch 44544~45055/48000 | Train loss: 0.1580\n",
      "Epoch: 008/015 | Batch 45056~45567/48000 | Train loss: 0.1593\n",
      "Epoch: 008/015 | Batch 45568~46079/48000 | Train loss: 0.1803\n",
      "Epoch: 008/015 | Batch 46080~46591/48000 | Train loss: 0.1356\n",
      "Epoch: 008/015 | Batch 46592~47103/48000 | Train loss: 0.1722\n",
      "Epoch: 008/015 | Batch 47104~47615/48000 | Train loss: 0.1434\n",
      "Epoch: 008/015 | Batch 47616~48127/48000 | Train loss: 0.1469\n",
      "Epoch: 008/015 | Batch 000~511/12000 | Validation loss: 0.1663\n",
      "Epoch: 008/015 | Batch 512~1023/12000 | Validation loss: 0.1369\n",
      "Epoch: 008/015 | Batch 1024~1535/12000 | Validation loss: 0.1133\n",
      "Epoch: 008/015 | Batch 1536~2047/12000 | Validation loss: 0.1403\n",
      "Epoch: 008/015 | Batch 2048~2559/12000 | Validation loss: 0.1840\n",
      "Epoch: 008/015 | Batch 2560~3071/12000 | Validation loss: 0.1246\n",
      "Epoch: 008/015 | Batch 3072~3583/12000 | Validation loss: 0.1851\n",
      "Epoch: 008/015 | Batch 3584~4095/12000 | Validation loss: 0.1590\n",
      "Epoch: 008/015 | Batch 4096~4607/12000 | Validation loss: 0.1281\n",
      "Epoch: 008/015 | Batch 4608~5119/12000 | Validation loss: 0.1748\n",
      "Epoch: 008/015 | Batch 5120~5631/12000 | Validation loss: 0.2127\n",
      "Epoch: 008/015 | Batch 5632~6143/12000 | Validation loss: 0.1816\n",
      "Epoch: 008/015 | Batch 6144~6655/12000 | Validation loss: 0.1792\n",
      "Epoch: 008/015 | Batch 6656~7167/12000 | Validation loss: 0.1853\n",
      "Epoch: 008/015 | Batch 7168~7679/12000 | Validation loss: 0.1568\n",
      "Epoch: 008/015 | Batch 7680~8191/12000 | Validation loss: 0.1323\n",
      "Epoch: 008/015 | Batch 8192~8703/12000 | Validation loss: 0.1541\n",
      "Epoch: 008/015 | Batch 8704~9215/12000 | Validation loss: 0.1866\n",
      "Epoch: 008/015 | Batch 9216~9727/12000 | Validation loss: 0.1368\n",
      "Epoch: 008/015 | Batch 9728~10239/12000 | Validation loss: 0.1140\n",
      "Epoch: 008/015 | Batch 10240~10751/12000 | Validation loss: 0.1632\n",
      "Epoch: 008/015 | Batch 10752~11263/12000 | Validation loss: 0.1729\n",
      "Epoch: 008/015 | Batch 11264~11775/12000 | Validation loss: 0.1441\n",
      "Epoch: 008/015 | Batch 11776~12287/12000 | Validation loss: 0.1759\n",
      "Epoch: 008/015 | Train loss: 0.1692 | Validation loss: 0.1587 \n",
      "Epoch: 008/015 | Train error: 0.0531 | Validation error: 0.0481 \n",
      "Time elapsed: 49.15 min\n",
      "Epoch: 009/015 | Batch 000~511/48000 | Train loss: 0.1663\n",
      "Epoch: 009/015 | Batch 512~1023/48000 | Train loss: 0.1591\n",
      "Epoch: 009/015 | Batch 1024~1535/48000 | Train loss: 0.1066\n",
      "Epoch: 009/015 | Batch 1536~2047/48000 | Train loss: 0.1504\n",
      "Epoch: 009/015 | Batch 2048~2559/48000 | Train loss: 0.1259\n",
      "Epoch: 009/015 | Batch 2560~3071/48000 | Train loss: 0.1409\n",
      "Epoch: 009/015 | Batch 3072~3583/48000 | Train loss: 0.1063\n",
      "Epoch: 009/015 | Batch 3584~4095/48000 | Train loss: 0.1865\n",
      "Epoch: 009/015 | Batch 4096~4607/48000 | Train loss: 0.1388\n",
      "Epoch: 009/015 | Batch 4608~5119/48000 | Train loss: 0.1879\n",
      "Epoch: 009/015 | Batch 5120~5631/48000 | Train loss: 0.1589\n",
      "Epoch: 009/015 | Batch 5632~6143/48000 | Train loss: 0.1455\n",
      "Epoch: 009/015 | Batch 6144~6655/48000 | Train loss: 0.1469\n",
      "Epoch: 009/015 | Batch 6656~7167/48000 | Train loss: 0.1122\n",
      "Epoch: 009/015 | Batch 7168~7679/48000 | Train loss: 0.2105\n",
      "Epoch: 009/015 | Batch 7680~8191/48000 | Train loss: 0.1578\n",
      "Epoch: 009/015 | Batch 8192~8703/48000 | Train loss: 0.1743\n",
      "Epoch: 009/015 | Batch 8704~9215/48000 | Train loss: 0.1376\n",
      "Epoch: 009/015 | Batch 9216~9727/48000 | Train loss: 0.1658\n",
      "Epoch: 009/015 | Batch 9728~10239/48000 | Train loss: 0.1562\n",
      "Epoch: 009/015 | Batch 10240~10751/48000 | Train loss: 0.1836\n",
      "Epoch: 009/015 | Batch 10752~11263/48000 | Train loss: 0.1356\n",
      "Epoch: 009/015 | Batch 11264~11775/48000 | Train loss: 0.1187\n",
      "Epoch: 009/015 | Batch 11776~12287/48000 | Train loss: 0.1299\n",
      "Epoch: 009/015 | Batch 12288~12799/48000 | Train loss: 0.1254\n",
      "Epoch: 009/015 | Batch 12800~13311/48000 | Train loss: 0.1577\n",
      "Epoch: 009/015 | Batch 13312~13823/48000 | Train loss: 0.1213\n",
      "Epoch: 009/015 | Batch 13824~14335/48000 | Train loss: 0.1305\n",
      "Epoch: 009/015 | Batch 14336~14847/48000 | Train loss: 0.1464\n",
      "Epoch: 009/015 | Batch 14848~15359/48000 | Train loss: 0.1443\n",
      "Epoch: 009/015 | Batch 15360~15871/48000 | Train loss: 0.1614\n",
      "Epoch: 009/015 | Batch 15872~16383/48000 | Train loss: 0.1533\n",
      "Epoch: 009/015 | Batch 16384~16895/48000 | Train loss: 0.1369\n",
      "Epoch: 009/015 | Batch 16896~17407/48000 | Train loss: 0.1464\n",
      "Epoch: 009/015 | Batch 17408~17919/48000 | Train loss: 0.1349\n",
      "Epoch: 009/015 | Batch 17920~18431/48000 | Train loss: 0.1330\n",
      "Epoch: 009/015 | Batch 18432~18943/48000 | Train loss: 0.1463\n",
      "Epoch: 009/015 | Batch 18944~19455/48000 | Train loss: 0.1703\n",
      "Epoch: 009/015 | Batch 19456~19967/48000 | Train loss: 0.1831\n",
      "Epoch: 009/015 | Batch 19968~20479/48000 | Train loss: 0.1296\n",
      "Epoch: 009/015 | Batch 20480~20991/48000 | Train loss: 0.1175\n",
      "Epoch: 009/015 | Batch 20992~21503/48000 | Train loss: 0.1660\n",
      "Epoch: 009/015 | Batch 21504~22015/48000 | Train loss: 0.1442\n",
      "Epoch: 009/015 | Batch 22016~22527/48000 | Train loss: 0.1455\n",
      "Epoch: 009/015 | Batch 22528~23039/48000 | Train loss: 0.1358\n",
      "Epoch: 009/015 | Batch 23040~23551/48000 | Train loss: 0.1035\n",
      "Epoch: 009/015 | Batch 23552~24063/48000 | Train loss: 0.1207\n",
      "Epoch: 009/015 | Batch 24064~24575/48000 | Train loss: 0.1589\n",
      "Epoch: 009/015 | Batch 24576~25087/48000 | Train loss: 0.1512\n",
      "Epoch: 009/015 | Batch 25088~25599/48000 | Train loss: 0.1482\n",
      "Epoch: 009/015 | Batch 25600~26111/48000 | Train loss: 0.1725\n",
      "Epoch: 009/015 | Batch 26112~26623/48000 | Train loss: 0.1505\n",
      "Epoch: 009/015 | Batch 26624~27135/48000 | Train loss: 0.1056\n",
      "Epoch: 009/015 | Batch 27136~27647/48000 | Train loss: 0.1122\n",
      "Epoch: 009/015 | Batch 27648~28159/48000 | Train loss: 0.1225\n",
      "Epoch: 009/015 | Batch 28160~28671/48000 | Train loss: 0.1053\n",
      "Epoch: 009/015 | Batch 28672~29183/48000 | Train loss: 0.1294\n",
      "Epoch: 009/015 | Batch 29184~29695/48000 | Train loss: 0.1644\n",
      "Epoch: 009/015 | Batch 29696~30207/48000 | Train loss: 0.1258\n",
      "Epoch: 009/015 | Batch 30208~30719/48000 | Train loss: 0.1282\n",
      "Epoch: 009/015 | Batch 30720~31231/48000 | Train loss: 0.1279\n",
      "Epoch: 009/015 | Batch 31232~31743/48000 | Train loss: 0.1824\n",
      "Epoch: 009/015 | Batch 31744~32255/48000 | Train loss: 0.1168\n",
      "Epoch: 009/015 | Batch 32256~32767/48000 | Train loss: 0.1282\n",
      "Epoch: 009/015 | Batch 32768~33279/48000 | Train loss: 0.1600\n",
      "Epoch: 009/015 | Batch 33280~33791/48000 | Train loss: 0.1452\n",
      "Epoch: 009/015 | Batch 33792~34303/48000 | Train loss: 0.1180\n",
      "Epoch: 009/015 | Batch 34304~34815/48000 | Train loss: 0.1134\n",
      "Epoch: 009/015 | Batch 34816~35327/48000 | Train loss: 0.1410\n",
      "Epoch: 009/015 | Batch 35328~35839/48000 | Train loss: 0.1383\n",
      "Epoch: 009/015 | Batch 35840~36351/48000 | Train loss: 0.1444\n",
      "Epoch: 009/015 | Batch 36352~36863/48000 | Train loss: 0.1504\n",
      "Epoch: 009/015 | Batch 36864~37375/48000 | Train loss: 0.1698\n",
      "Epoch: 009/015 | Batch 37376~37887/48000 | Train loss: 0.1832\n",
      "Epoch: 009/015 | Batch 37888~38399/48000 | Train loss: 0.0758\n",
      "Epoch: 009/015 | Batch 38400~38911/48000 | Train loss: 0.1594\n",
      "Epoch: 009/015 | Batch 38912~39423/48000 | Train loss: 0.1511\n",
      "Epoch: 009/015 | Batch 39424~39935/48000 | Train loss: 0.1310\n",
      "Epoch: 009/015 | Batch 39936~40447/48000 | Train loss: 0.1502\n",
      "Epoch: 009/015 | Batch 40448~40959/48000 | Train loss: 0.1394\n",
      "Epoch: 009/015 | Batch 40960~41471/48000 | Train loss: 0.1119\n",
      "Epoch: 009/015 | Batch 41472~41983/48000 | Train loss: 0.1309\n",
      "Epoch: 009/015 | Batch 41984~42495/48000 | Train loss: 0.1410\n",
      "Epoch: 009/015 | Batch 42496~43007/48000 | Train loss: 0.1263\n",
      "Epoch: 009/015 | Batch 43008~43519/48000 | Train loss: 0.1779\n",
      "Epoch: 009/015 | Batch 43520~44031/48000 | Train loss: 0.1778\n",
      "Epoch: 009/015 | Batch 44032~44543/48000 | Train loss: 0.1094\n",
      "Epoch: 009/015 | Batch 44544~45055/48000 | Train loss: 0.1513\n",
      "Epoch: 009/015 | Batch 45056~45567/48000 | Train loss: 0.1251\n",
      "Epoch: 009/015 | Batch 45568~46079/48000 | Train loss: 0.1327\n",
      "Epoch: 009/015 | Batch 46080~46591/48000 | Train loss: 0.0817\n",
      "Epoch: 009/015 | Batch 46592~47103/48000 | Train loss: 0.1167\n",
      "Epoch: 009/015 | Batch 47104~47615/48000 | Train loss: 0.1213\n",
      "Epoch: 009/015 | Batch 47616~48127/48000 | Train loss: 0.1439\n",
      "Epoch: 009/015 | Batch 000~511/12000 | Validation loss: 0.1347\n",
      "Epoch: 009/015 | Batch 512~1023/12000 | Validation loss: 0.1286\n",
      "Epoch: 009/015 | Batch 1024~1535/12000 | Validation loss: 0.0978\n",
      "Epoch: 009/015 | Batch 1536~2047/12000 | Validation loss: 0.1292\n",
      "Epoch: 009/015 | Batch 2048~2559/12000 | Validation loss: 0.1482\n",
      "Epoch: 009/015 | Batch 2560~3071/12000 | Validation loss: 0.1321\n",
      "Epoch: 009/015 | Batch 3072~3583/12000 | Validation loss: 0.1681\n",
      "Epoch: 009/015 | Batch 3584~4095/12000 | Validation loss: 0.1371\n",
      "Epoch: 009/015 | Batch 4096~4607/12000 | Validation loss: 0.0971\n",
      "Epoch: 009/015 | Batch 4608~5119/12000 | Validation loss: 0.1677\n",
      "Epoch: 009/015 | Batch 5120~5631/12000 | Validation loss: 0.2032\n",
      "Epoch: 009/015 | Batch 5632~6143/12000 | Validation loss: 0.1537\n",
      "Epoch: 009/015 | Batch 6144~6655/12000 | Validation loss: 0.1448\n",
      "Epoch: 009/015 | Batch 6656~7167/12000 | Validation loss: 0.1664\n",
      "Epoch: 009/015 | Batch 7168~7679/12000 | Validation loss: 0.1474\n",
      "Epoch: 009/015 | Batch 7680~8191/12000 | Validation loss: 0.1353\n",
      "Epoch: 009/015 | Batch 8192~8703/12000 | Validation loss: 0.1252\n",
      "Epoch: 009/015 | Batch 8704~9215/12000 | Validation loss: 0.1556\n",
      "Epoch: 009/015 | Batch 9216~9727/12000 | Validation loss: 0.1359\n",
      "Epoch: 009/015 | Batch 9728~10239/12000 | Validation loss: 0.1130\n",
      "Epoch: 009/015 | Batch 10240~10751/12000 | Validation loss: 0.1480\n",
      "Epoch: 009/015 | Batch 10752~11263/12000 | Validation loss: 0.1654\n",
      "Epoch: 009/015 | Batch 11264~11775/12000 | Validation loss: 0.1267\n",
      "Epoch: 009/015 | Batch 11776~12287/12000 | Validation loss: 0.1877\n",
      "Epoch: 009/015 | Train loss: 0.1415 | Validation loss: 0.1437 \n",
      "Epoch: 009/015 | Train error: 0.0431 | Validation error: 0.0437 \n",
      "Time elapsed: 49.10 min\n",
      "Epoch: 010/015 | Batch 000~511/48000 | Train loss: 0.1435\n",
      "Epoch: 010/015 | Batch 512~1023/48000 | Train loss: 0.1134\n",
      "Epoch: 010/015 | Batch 1024~1535/48000 | Train loss: 0.1576\n",
      "Epoch: 010/015 | Batch 1536~2047/48000 | Train loss: 0.1001\n",
      "Epoch: 010/015 | Batch 2048~2559/48000 | Train loss: 0.1006\n",
      "Epoch: 010/015 | Batch 2560~3071/48000 | Train loss: 0.1435\n",
      "Epoch: 010/015 | Batch 3072~3583/48000 | Train loss: 0.0818\n",
      "Epoch: 010/015 | Batch 3584~4095/48000 | Train loss: 0.1369\n",
      "Epoch: 010/015 | Batch 4096~4607/48000 | Train loss: 0.1011\n",
      "Epoch: 010/015 | Batch 4608~5119/48000 | Train loss: 0.1347\n",
      "Epoch: 010/015 | Batch 5120~5631/48000 | Train loss: 0.1223\n",
      "Epoch: 010/015 | Batch 5632~6143/48000 | Train loss: 0.1676\n",
      "Epoch: 010/015 | Batch 6144~6655/48000 | Train loss: 0.1698\n",
      "Epoch: 010/015 | Batch 6656~7167/48000 | Train loss: 0.1309\n",
      "Epoch: 010/015 | Batch 7168~7679/48000 | Train loss: 0.1206\n",
      "Epoch: 010/015 | Batch 7680~8191/48000 | Train loss: 0.1790\n",
      "Epoch: 010/015 | Batch 8192~8703/48000 | Train loss: 0.1226\n",
      "Epoch: 010/015 | Batch 8704~9215/48000 | Train loss: 0.1192\n",
      "Epoch: 010/015 | Batch 9216~9727/48000 | Train loss: 0.1305\n",
      "Epoch: 010/015 | Batch 9728~10239/48000 | Train loss: 0.1409\n",
      "Epoch: 010/015 | Batch 10240~10751/48000 | Train loss: 0.1441\n",
      "Epoch: 010/015 | Batch 10752~11263/48000 | Train loss: 0.1364\n",
      "Epoch: 010/015 | Batch 11264~11775/48000 | Train loss: 0.1296\n",
      "Epoch: 010/015 | Batch 11776~12287/48000 | Train loss: 0.1439\n",
      "Epoch: 010/015 | Batch 12288~12799/48000 | Train loss: 0.1445\n",
      "Epoch: 010/015 | Batch 12800~13311/48000 | Train loss: 0.1047\n",
      "Epoch: 010/015 | Batch 13312~13823/48000 | Train loss: 0.1396\n",
      "Epoch: 010/015 | Batch 13824~14335/48000 | Train loss: 0.1448\n",
      "Epoch: 010/015 | Batch 14336~14847/48000 | Train loss: 0.1382\n",
      "Epoch: 010/015 | Batch 14848~15359/48000 | Train loss: 0.1383\n",
      "Epoch: 010/015 | Batch 15360~15871/48000 | Train loss: 0.1248\n",
      "Epoch: 010/015 | Batch 15872~16383/48000 | Train loss: 0.1430\n",
      "Epoch: 010/015 | Batch 16384~16895/48000 | Train loss: 0.1721\n",
      "Epoch: 010/015 | Batch 16896~17407/48000 | Train loss: 0.0920\n",
      "Epoch: 010/015 | Batch 17408~17919/48000 | Train loss: 0.1198\n",
      "Epoch: 010/015 | Batch 17920~18431/48000 | Train loss: 0.1278\n",
      "Epoch: 010/015 | Batch 18432~18943/48000 | Train loss: 0.1605\n",
      "Epoch: 010/015 | Batch 18944~19455/48000 | Train loss: 0.1637\n",
      "Epoch: 010/015 | Batch 19456~19967/48000 | Train loss: 0.0947\n",
      "Epoch: 010/015 | Batch 19968~20479/48000 | Train loss: 0.1050\n",
      "Epoch: 010/015 | Batch 20480~20991/48000 | Train loss: 0.1265\n",
      "Epoch: 010/015 | Batch 20992~21503/48000 | Train loss: 0.1270\n",
      "Epoch: 010/015 | Batch 21504~22015/48000 | Train loss: 0.1061\n",
      "Epoch: 010/015 | Batch 22016~22527/48000 | Train loss: 0.1669\n",
      "Epoch: 010/015 | Batch 22528~23039/48000 | Train loss: 0.1445\n",
      "Epoch: 010/015 | Batch 23040~23551/48000 | Train loss: 0.1259\n",
      "Epoch: 010/015 | Batch 23552~24063/48000 | Train loss: 0.1332\n",
      "Epoch: 010/015 | Batch 24064~24575/48000 | Train loss: 0.1511\n",
      "Epoch: 010/015 | Batch 24576~25087/48000 | Train loss: 0.1451\n",
      "Epoch: 010/015 | Batch 25088~25599/48000 | Train loss: 0.1609\n",
      "Epoch: 010/015 | Batch 25600~26111/48000 | Train loss: 0.0860\n",
      "Epoch: 010/015 | Batch 26112~26623/48000 | Train loss: 0.1428\n",
      "Epoch: 010/015 | Batch 26624~27135/48000 | Train loss: 0.1389\n",
      "Epoch: 010/015 | Batch 27136~27647/48000 | Train loss: 0.1309\n",
      "Epoch: 010/015 | Batch 27648~28159/48000 | Train loss: 0.1537\n",
      "Epoch: 010/015 | Batch 28160~28671/48000 | Train loss: 0.0971\n",
      "Epoch: 010/015 | Batch 28672~29183/48000 | Train loss: 0.1399\n",
      "Epoch: 010/015 | Batch 29184~29695/48000 | Train loss: 0.1047\n",
      "Epoch: 010/015 | Batch 29696~30207/48000 | Train loss: 0.1275\n",
      "Epoch: 010/015 | Batch 30208~30719/48000 | Train loss: 0.1255\n",
      "Epoch: 010/015 | Batch 30720~31231/48000 | Train loss: 0.0946\n",
      "Epoch: 010/015 | Batch 31232~31743/48000 | Train loss: 0.1058\n",
      "Epoch: 010/015 | Batch 31744~32255/48000 | Train loss: 0.1672\n",
      "Epoch: 010/015 | Batch 32256~32767/48000 | Train loss: 0.1216\n",
      "Epoch: 010/015 | Batch 32768~33279/48000 | Train loss: 0.1565\n",
      "Epoch: 010/015 | Batch 33280~33791/48000 | Train loss: 0.1090\n",
      "Epoch: 010/015 | Batch 33792~34303/48000 | Train loss: 0.2159\n",
      "Epoch: 010/015 | Batch 34304~34815/48000 | Train loss: 0.1885\n",
      "Epoch: 010/015 | Batch 34816~35327/48000 | Train loss: 0.1834\n",
      "Epoch: 010/015 | Batch 35328~35839/48000 | Train loss: 0.1190\n",
      "Epoch: 010/015 | Batch 35840~36351/48000 | Train loss: 0.1229\n",
      "Epoch: 010/015 | Batch 36352~36863/48000 | Train loss: 0.1645\n",
      "Epoch: 010/015 | Batch 36864~37375/48000 | Train loss: 0.1080\n",
      "Epoch: 010/015 | Batch 37376~37887/48000 | Train loss: 0.1165\n",
      "Epoch: 010/015 | Batch 37888~38399/48000 | Train loss: 0.1348\n",
      "Epoch: 010/015 | Batch 38400~38911/48000 | Train loss: 0.1745\n",
      "Epoch: 010/015 | Batch 38912~39423/48000 | Train loss: 0.1110\n",
      "Epoch: 010/015 | Batch 39424~39935/48000 | Train loss: 0.1287\n",
      "Epoch: 010/015 | Batch 39936~40447/48000 | Train loss: 0.0912\n",
      "Epoch: 010/015 | Batch 40448~40959/48000 | Train loss: 0.0987\n",
      "Epoch: 010/015 | Batch 40960~41471/48000 | Train loss: 0.1251\n",
      "Epoch: 010/015 | Batch 41472~41983/48000 | Train loss: 0.1135\n",
      "Epoch: 010/015 | Batch 41984~42495/48000 | Train loss: 0.1278\n",
      "Epoch: 010/015 | Batch 42496~43007/48000 | Train loss: 0.1226\n",
      "Epoch: 010/015 | Batch 43008~43519/48000 | Train loss: 0.1462\n",
      "Epoch: 010/015 | Batch 43520~44031/48000 | Train loss: 0.1403\n",
      "Epoch: 010/015 | Batch 44032~44543/48000 | Train loss: 0.1523\n",
      "Epoch: 010/015 | Batch 44544~45055/48000 | Train loss: 0.1596\n",
      "Epoch: 010/015 | Batch 45056~45567/48000 | Train loss: 0.1029\n",
      "Epoch: 010/015 | Batch 45568~46079/48000 | Train loss: 0.1118\n",
      "Epoch: 010/015 | Batch 46080~46591/48000 | Train loss: 0.1431\n",
      "Epoch: 010/015 | Batch 46592~47103/48000 | Train loss: 0.1402\n",
      "Epoch: 010/015 | Batch 47104~47615/48000 | Train loss: 0.1774\n",
      "Epoch: 010/015 | Batch 47616~48127/48000 | Train loss: 0.0999\n",
      "Epoch: 010/015 | Batch 000~511/12000 | Validation loss: 0.1176\n",
      "Epoch: 010/015 | Batch 512~1023/12000 | Validation loss: 0.1253\n",
      "Epoch: 010/015 | Batch 1024~1535/12000 | Validation loss: 0.0903\n",
      "Epoch: 010/015 | Batch 1536~2047/12000 | Validation loss: 0.1018\n",
      "Epoch: 010/015 | Batch 2048~2559/12000 | Validation loss: 0.1343\n",
      "Epoch: 010/015 | Batch 2560~3071/12000 | Validation loss: 0.1223\n",
      "Epoch: 010/015 | Batch 3072~3583/12000 | Validation loss: 0.1392\n",
      "Epoch: 010/015 | Batch 3584~4095/12000 | Validation loss: 0.1161\n",
      "Epoch: 010/015 | Batch 4096~4607/12000 | Validation loss: 0.1157\n",
      "Epoch: 010/015 | Batch 4608~5119/12000 | Validation loss: 0.1413\n",
      "Epoch: 010/015 | Batch 5120~5631/12000 | Validation loss: 0.2108\n",
      "Epoch: 010/015 | Batch 5632~6143/12000 | Validation loss: 0.1706\n",
      "Epoch: 010/015 | Batch 6144~6655/12000 | Validation loss: 0.1343\n",
      "Epoch: 010/015 | Batch 6656~7167/12000 | Validation loss: 0.1713\n",
      "Epoch: 010/015 | Batch 7168~7679/12000 | Validation loss: 0.1546\n",
      "Epoch: 010/015 | Batch 7680~8191/12000 | Validation loss: 0.1181\n",
      "Epoch: 010/015 | Batch 8192~8703/12000 | Validation loss: 0.1417\n",
      "Epoch: 010/015 | Batch 8704~9215/12000 | Validation loss: 0.1398\n",
      "Epoch: 010/015 | Batch 9216~9727/12000 | Validation loss: 0.1147\n",
      "Epoch: 010/015 | Batch 9728~10239/12000 | Validation loss: 0.1100\n",
      "Epoch: 010/015 | Batch 10240~10751/12000 | Validation loss: 0.1457\n",
      "Epoch: 010/015 | Batch 10752~11263/12000 | Validation loss: 0.1578\n",
      "Epoch: 010/015 | Batch 11264~11775/12000 | Validation loss: 0.1156\n",
      "Epoch: 010/015 | Batch 11776~12287/12000 | Validation loss: 0.1749\n",
      "Epoch: 010/015 | Train loss: 0.1329 | Validation loss: 0.1360 \n",
      "Epoch: 010/015 | Train error: 0.0396 | Validation error: 0.0409 \n",
      "Time elapsed: 49.05 min\n",
      "Epoch: 011/015 | Batch 000~511/48000 | Train loss: 0.1398\n",
      "Epoch: 011/015 | Batch 512~1023/48000 | Train loss: 0.1557\n",
      "Epoch: 011/015 | Batch 1024~1535/48000 | Train loss: 0.1368\n",
      "Epoch: 011/015 | Batch 1536~2047/48000 | Train loss: 0.1437\n",
      "Epoch: 011/015 | Batch 2048~2559/48000 | Train loss: 0.1115\n",
      "Epoch: 011/015 | Batch 2560~3071/48000 | Train loss: 0.0953\n",
      "Epoch: 011/015 | Batch 3072~3583/48000 | Train loss: 0.1322\n",
      "Epoch: 011/015 | Batch 3584~4095/48000 | Train loss: 0.1189\n",
      "Epoch: 011/015 | Batch 4096~4607/48000 | Train loss: 0.1100\n",
      "Epoch: 011/015 | Batch 4608~5119/48000 | Train loss: 0.0933\n",
      "Epoch: 011/015 | Batch 5120~5631/48000 | Train loss: 0.0932\n",
      "Epoch: 011/015 | Batch 5632~6143/48000 | Train loss: 0.1399\n",
      "Epoch: 011/015 | Batch 6144~6655/48000 | Train loss: 0.1436\n",
      "Epoch: 011/015 | Batch 6656~7167/48000 | Train loss: 0.1208\n",
      "Epoch: 011/015 | Batch 7168~7679/48000 | Train loss: 0.0645\n",
      "Epoch: 011/015 | Batch 7680~8191/48000 | Train loss: 0.1396\n",
      "Epoch: 011/015 | Batch 8192~8703/48000 | Train loss: 0.1011\n",
      "Epoch: 011/015 | Batch 8704~9215/48000 | Train loss: 0.1443\n",
      "Epoch: 011/015 | Batch 9216~9727/48000 | Train loss: 0.1047\n",
      "Epoch: 011/015 | Batch 9728~10239/48000 | Train loss: 0.0872\n",
      "Epoch: 011/015 | Batch 10240~10751/48000 | Train loss: 0.1321\n",
      "Epoch: 011/015 | Batch 10752~11263/48000 | Train loss: 0.1344\n",
      "Epoch: 011/015 | Batch 11264~11775/48000 | Train loss: 0.1553\n",
      "Epoch: 011/015 | Batch 11776~12287/48000 | Train loss: 0.1611\n",
      "Epoch: 011/015 | Batch 12288~12799/48000 | Train loss: 0.1425\n",
      "Epoch: 011/015 | Batch 12800~13311/48000 | Train loss: 0.1081\n",
      "Epoch: 011/015 | Batch 13312~13823/48000 | Train loss: 0.1376\n",
      "Epoch: 011/015 | Batch 13824~14335/48000 | Train loss: 0.1174\n",
      "Epoch: 011/015 | Batch 14336~14847/48000 | Train loss: 0.1572\n",
      "Epoch: 011/015 | Batch 14848~15359/48000 | Train loss: 0.0943\n",
      "Epoch: 011/015 | Batch 15360~15871/48000 | Train loss: 0.1341\n",
      "Epoch: 011/015 | Batch 15872~16383/48000 | Train loss: 0.1138\n",
      "Epoch: 011/015 | Batch 16384~16895/48000 | Train loss: 0.1302\n",
      "Epoch: 011/015 | Batch 16896~17407/48000 | Train loss: 0.1267\n",
      "Epoch: 011/015 | Batch 17408~17919/48000 | Train loss: 0.1199\n",
      "Epoch: 011/015 | Batch 17920~18431/48000 | Train loss: 0.0999\n",
      "Epoch: 011/015 | Batch 18432~18943/48000 | Train loss: 0.1156\n",
      "Epoch: 011/015 | Batch 18944~19455/48000 | Train loss: 0.1147\n",
      "Epoch: 011/015 | Batch 19456~19967/48000 | Train loss: 0.1017\n",
      "Epoch: 011/015 | Batch 19968~20479/48000 | Train loss: 0.1051\n",
      "Epoch: 011/015 | Batch 20480~20991/48000 | Train loss: 0.1317\n",
      "Epoch: 011/015 | Batch 20992~21503/48000 | Train loss: 0.1382\n",
      "Epoch: 011/015 | Batch 21504~22015/48000 | Train loss: 0.1342\n",
      "Epoch: 011/015 | Batch 22016~22527/48000 | Train loss: 0.1142\n",
      "Epoch: 011/015 | Batch 22528~23039/48000 | Train loss: 0.1540\n",
      "Epoch: 011/015 | Batch 23040~23551/48000 | Train loss: 0.0873\n",
      "Epoch: 011/015 | Batch 23552~24063/48000 | Train loss: 0.1348\n",
      "Epoch: 011/015 | Batch 24064~24575/48000 | Train loss: 0.1420\n",
      "Epoch: 011/015 | Batch 24576~25087/48000 | Train loss: 0.1238\n",
      "Epoch: 011/015 | Batch 25088~25599/48000 | Train loss: 0.1208\n",
      "Epoch: 011/015 | Batch 25600~26111/48000 | Train loss: 0.1432\n",
      "Epoch: 011/015 | Batch 26112~26623/48000 | Train loss: 0.1052\n",
      "Epoch: 011/015 | Batch 26624~27135/48000 | Train loss: 0.1417\n",
      "Epoch: 011/015 | Batch 27136~27647/48000 | Train loss: 0.1221\n",
      "Epoch: 011/015 | Batch 27648~28159/48000 | Train loss: 0.1024\n",
      "Epoch: 011/015 | Batch 28160~28671/48000 | Train loss: 0.0772\n",
      "Epoch: 011/015 | Batch 28672~29183/48000 | Train loss: 0.1955\n",
      "Epoch: 011/015 | Batch 29184~29695/48000 | Train loss: 0.1305\n",
      "Epoch: 011/015 | Batch 29696~30207/48000 | Train loss: 0.1125\n",
      "Epoch: 011/015 | Batch 30208~30719/48000 | Train loss: 0.1286\n",
      "Epoch: 011/015 | Batch 30720~31231/48000 | Train loss: 0.1112\n",
      "Epoch: 011/015 | Batch 31232~31743/48000 | Train loss: 0.0878\n",
      "Epoch: 011/015 | Batch 31744~32255/48000 | Train loss: 0.1207\n",
      "Epoch: 011/015 | Batch 32256~32767/48000 | Train loss: 0.1351\n",
      "Epoch: 011/015 | Batch 32768~33279/48000 | Train loss: 0.0835\n",
      "Epoch: 011/015 | Batch 33280~33791/48000 | Train loss: 0.1266\n",
      "Epoch: 011/015 | Batch 33792~34303/48000 | Train loss: 0.1801\n",
      "Epoch: 011/015 | Batch 34304~34815/48000 | Train loss: 0.1355\n",
      "Epoch: 011/015 | Batch 34816~35327/48000 | Train loss: 0.1214\n",
      "Epoch: 011/015 | Batch 35328~35839/48000 | Train loss: 0.1078\n",
      "Epoch: 011/015 | Batch 35840~36351/48000 | Train loss: 0.1050\n",
      "Epoch: 011/015 | Batch 36352~36863/48000 | Train loss: 0.0913\n",
      "Epoch: 011/015 | Batch 36864~37375/48000 | Train loss: 0.1307\n",
      "Epoch: 011/015 | Batch 37376~37887/48000 | Train loss: 0.1143\n",
      "Epoch: 011/015 | Batch 37888~38399/48000 | Train loss: 0.1527\n",
      "Epoch: 011/015 | Batch 38400~38911/48000 | Train loss: 0.1117\n",
      "Epoch: 011/015 | Batch 38912~39423/48000 | Train loss: 0.1019\n",
      "Epoch: 011/015 | Batch 39424~39935/48000 | Train loss: 0.1379\n",
      "Epoch: 011/015 | Batch 39936~40447/48000 | Train loss: 0.1447\n",
      "Epoch: 011/015 | Batch 40448~40959/48000 | Train loss: 0.1383\n",
      "Epoch: 011/015 | Batch 40960~41471/48000 | Train loss: 0.1276\n",
      "Epoch: 011/015 | Batch 41472~41983/48000 | Train loss: 0.0977\n",
      "Epoch: 011/015 | Batch 41984~42495/48000 | Train loss: 0.1291\n",
      "Epoch: 011/015 | Batch 42496~43007/48000 | Train loss: 0.1081\n",
      "Epoch: 011/015 | Batch 43008~43519/48000 | Train loss: 0.0718\n",
      "Epoch: 011/015 | Batch 43520~44031/48000 | Train loss: 0.1755\n",
      "Epoch: 011/015 | Batch 44032~44543/48000 | Train loss: 0.1786\n",
      "Epoch: 011/015 | Batch 44544~45055/48000 | Train loss: 0.1678\n",
      "Epoch: 011/015 | Batch 45056~45567/48000 | Train loss: 0.0957\n",
      "Epoch: 011/015 | Batch 45568~46079/48000 | Train loss: 0.1570\n",
      "Epoch: 011/015 | Batch 46080~46591/48000 | Train loss: 0.1392\n",
      "Epoch: 011/015 | Batch 46592~47103/48000 | Train loss: 0.1304\n",
      "Epoch: 011/015 | Batch 47104~47615/48000 | Train loss: 0.1248\n",
      "Epoch: 011/015 | Batch 47616~48127/48000 | Train loss: 0.1267\n",
      "Epoch: 011/015 | Batch 000~511/12000 | Validation loss: 0.1233\n",
      "Epoch: 011/015 | Batch 512~1023/12000 | Validation loss: 0.0983\n",
      "Epoch: 011/015 | Batch 1024~1535/12000 | Validation loss: 0.0807\n",
      "Epoch: 011/015 | Batch 1536~2047/12000 | Validation loss: 0.1142\n",
      "Epoch: 011/015 | Batch 2048~2559/12000 | Validation loss: 0.1196\n",
      "Epoch: 011/015 | Batch 2560~3071/12000 | Validation loss: 0.1017\n",
      "Epoch: 011/015 | Batch 3072~3583/12000 | Validation loss: 0.1458\n",
      "Epoch: 011/015 | Batch 3584~4095/12000 | Validation loss: 0.1312\n",
      "Epoch: 011/015 | Batch 4096~4607/12000 | Validation loss: 0.0868\n",
      "Epoch: 011/015 | Batch 4608~5119/12000 | Validation loss: 0.1332\n",
      "Epoch: 011/015 | Batch 5120~5631/12000 | Validation loss: 0.1872\n",
      "Epoch: 011/015 | Batch 5632~6143/12000 | Validation loss: 0.1437\n",
      "Epoch: 011/015 | Batch 6144~6655/12000 | Validation loss: 0.1506\n",
      "Epoch: 011/015 | Batch 6656~7167/12000 | Validation loss: 0.1525\n",
      "Epoch: 011/015 | Batch 7168~7679/12000 | Validation loss: 0.1471\n",
      "Epoch: 011/015 | Batch 7680~8191/12000 | Validation loss: 0.1029\n",
      "Epoch: 011/015 | Batch 8192~8703/12000 | Validation loss: 0.1133\n",
      "Epoch: 011/015 | Batch 8704~9215/12000 | Validation loss: 0.1404\n",
      "Epoch: 011/015 | Batch 9216~9727/12000 | Validation loss: 0.1006\n",
      "Epoch: 011/015 | Batch 9728~10239/12000 | Validation loss: 0.1194\n",
      "Epoch: 011/015 | Batch 10240~10751/12000 | Validation loss: 0.1435\n",
      "Epoch: 011/015 | Batch 10752~11263/12000 | Validation loss: 0.1420\n",
      "Epoch: 011/015 | Batch 11264~11775/12000 | Validation loss: 0.1221\n",
      "Epoch: 011/015 | Batch 11776~12287/12000 | Validation loss: 0.1544\n",
      "Epoch: 011/015 | Train loss: 0.1243 | Validation loss: 0.1273 \n",
      "Epoch: 011/015 | Train error: 0.0376 | Validation error: 0.0388 \n",
      "Time elapsed: 49.06 min\n",
      "Epoch: 012/015 | Batch 000~511/48000 | Train loss: 0.1019\n",
      "Epoch: 012/015 | Batch 512~1023/48000 | Train loss: 0.1539\n",
      "Epoch: 012/015 | Batch 1024~1535/48000 | Train loss: 0.0982\n",
      "Epoch: 012/015 | Batch 1536~2047/48000 | Train loss: 0.0848\n",
      "Epoch: 012/015 | Batch 2048~2559/48000 | Train loss: 0.0882\n",
      "Epoch: 012/015 | Batch 2560~3071/48000 | Train loss: 0.1058\n",
      "Epoch: 012/015 | Batch 3072~3583/48000 | Train loss: 0.1053\n",
      "Epoch: 012/015 | Batch 3584~4095/48000 | Train loss: 0.1139\n",
      "Epoch: 012/015 | Batch 4096~4607/48000 | Train loss: 0.1049\n",
      "Epoch: 012/015 | Batch 4608~5119/48000 | Train loss: 0.1475\n",
      "Epoch: 012/015 | Batch 5120~5631/48000 | Train loss: 0.1159\n",
      "Epoch: 012/015 | Batch 5632~6143/48000 | Train loss: 0.1505\n",
      "Epoch: 012/015 | Batch 6144~6655/48000 | Train loss: 0.1364\n",
      "Epoch: 012/015 | Batch 6656~7167/48000 | Train loss: 0.1512\n",
      "Epoch: 012/015 | Batch 7168~7679/48000 | Train loss: 0.1020\n",
      "Epoch: 012/015 | Batch 7680~8191/48000 | Train loss: 0.1143\n",
      "Epoch: 012/015 | Batch 8192~8703/48000 | Train loss: 0.0967\n",
      "Epoch: 012/015 | Batch 8704~9215/48000 | Train loss: 0.1446\n",
      "Epoch: 012/015 | Batch 9216~9727/48000 | Train loss: 0.1067\n",
      "Epoch: 012/015 | Batch 9728~10239/48000 | Train loss: 0.0955\n",
      "Epoch: 012/015 | Batch 10240~10751/48000 | Train loss: 0.1223\n",
      "Epoch: 012/015 | Batch 10752~11263/48000 | Train loss: 0.0944\n",
      "Epoch: 012/015 | Batch 11264~11775/48000 | Train loss: 0.0888\n",
      "Epoch: 012/015 | Batch 11776~12287/48000 | Train loss: 0.1521\n",
      "Epoch: 012/015 | Batch 12288~12799/48000 | Train loss: 0.1378\n",
      "Epoch: 012/015 | Batch 12800~13311/48000 | Train loss: 0.1418\n",
      "Epoch: 012/015 | Batch 13312~13823/48000 | Train loss: 0.1145\n",
      "Epoch: 012/015 | Batch 13824~14335/48000 | Train loss: 0.1323\n",
      "Epoch: 012/015 | Batch 14336~14847/48000 | Train loss: 0.1145\n",
      "Epoch: 012/015 | Batch 14848~15359/48000 | Train loss: 0.0868\n",
      "Epoch: 012/015 | Batch 15360~15871/48000 | Train loss: 0.1178\n",
      "Epoch: 012/015 | Batch 15872~16383/48000 | Train loss: 0.1621\n",
      "Epoch: 012/015 | Batch 16384~16895/48000 | Train loss: 0.1111\n",
      "Epoch: 012/015 | Batch 16896~17407/48000 | Train loss: 0.1564\n",
      "Epoch: 012/015 | Batch 17408~17919/48000 | Train loss: 0.1125\n",
      "Epoch: 012/015 | Batch 17920~18431/48000 | Train loss: 0.1458\n",
      "Epoch: 012/015 | Batch 18432~18943/48000 | Train loss: 0.0988\n",
      "Epoch: 012/015 | Batch 18944~19455/48000 | Train loss: 0.1231\n",
      "Epoch: 012/015 | Batch 19456~19967/48000 | Train loss: 0.0812\n",
      "Epoch: 012/015 | Batch 19968~20479/48000 | Train loss: 0.1043\n",
      "Epoch: 012/015 | Batch 20480~20991/48000 | Train loss: 0.1353\n",
      "Epoch: 012/015 | Batch 20992~21503/48000 | Train loss: 0.1311\n",
      "Epoch: 012/015 | Batch 21504~22015/48000 | Train loss: 0.1082\n",
      "Epoch: 012/015 | Batch 22016~22527/48000 | Train loss: 0.1317\n",
      "Epoch: 012/015 | Batch 22528~23039/48000 | Train loss: 0.0832\n",
      "Epoch: 012/015 | Batch 23040~23551/48000 | Train loss: 0.0858\n",
      "Epoch: 012/015 | Batch 23552~24063/48000 | Train loss: 0.1140\n",
      "Epoch: 012/015 | Batch 24064~24575/48000 | Train loss: 0.1413\n",
      "Epoch: 012/015 | Batch 24576~25087/48000 | Train loss: 0.1133\n",
      "Epoch: 012/015 | Batch 25088~25599/48000 | Train loss: 0.1139\n",
      "Epoch: 012/015 | Batch 25600~26111/48000 | Train loss: 0.0980\n",
      "Epoch: 012/015 | Batch 26112~26623/48000 | Train loss: 0.1051\n",
      "Epoch: 012/015 | Batch 26624~27135/48000 | Train loss: 0.1146\n",
      "Epoch: 012/015 | Batch 27136~27647/48000 | Train loss: 0.1145\n",
      "Epoch: 012/015 | Batch 27648~28159/48000 | Train loss: 0.0850\n",
      "Epoch: 012/015 | Batch 28160~28671/48000 | Train loss: 0.1420\n",
      "Epoch: 012/015 | Batch 28672~29183/48000 | Train loss: 0.1236\n",
      "Epoch: 012/015 | Batch 29184~29695/48000 | Train loss: 0.1015\n",
      "Epoch: 012/015 | Batch 29696~30207/48000 | Train loss: 0.1203\n",
      "Epoch: 012/015 | Batch 30208~30719/48000 | Train loss: 0.1295\n",
      "Epoch: 012/015 | Batch 30720~31231/48000 | Train loss: 0.1427\n",
      "Epoch: 012/015 | Batch 31232~31743/48000 | Train loss: 0.1042\n",
      "Epoch: 012/015 | Batch 31744~32255/48000 | Train loss: 0.0983\n",
      "Epoch: 012/015 | Batch 32256~32767/48000 | Train loss: 0.1091\n",
      "Epoch: 012/015 | Batch 32768~33279/48000 | Train loss: 0.1492\n",
      "Epoch: 012/015 | Batch 33280~33791/48000 | Train loss: 0.1032\n",
      "Epoch: 012/015 | Batch 33792~34303/48000 | Train loss: 0.0870\n",
      "Epoch: 012/015 | Batch 34304~34815/48000 | Train loss: 0.0829\n",
      "Epoch: 012/015 | Batch 34816~35327/48000 | Train loss: 0.1330\n",
      "Epoch: 012/015 | Batch 35328~35839/48000 | Train loss: 0.1000\n",
      "Epoch: 012/015 | Batch 35840~36351/48000 | Train loss: 0.0983\n",
      "Epoch: 012/015 | Batch 36352~36863/48000 | Train loss: 0.1135\n",
      "Epoch: 012/015 | Batch 36864~37375/48000 | Train loss: 0.1174\n",
      "Epoch: 012/015 | Batch 37376~37887/48000 | Train loss: 0.1238\n",
      "Epoch: 012/015 | Batch 37888~38399/48000 | Train loss: 0.1180\n",
      "Epoch: 012/015 | Batch 38400~38911/48000 | Train loss: 0.1239\n",
      "Epoch: 012/015 | Batch 38912~39423/48000 | Train loss: 0.0920\n",
      "Epoch: 012/015 | Batch 39424~39935/48000 | Train loss: 0.1034\n",
      "Epoch: 012/015 | Batch 39936~40447/48000 | Train loss: 0.1238\n",
      "Epoch: 012/015 | Batch 40448~40959/48000 | Train loss: 0.1127\n",
      "Epoch: 012/015 | Batch 40960~41471/48000 | Train loss: 0.1050\n",
      "Epoch: 012/015 | Batch 41472~41983/48000 | Train loss: 0.1238\n",
      "Epoch: 012/015 | Batch 41984~42495/48000 | Train loss: 0.1293\n",
      "Epoch: 012/015 | Batch 42496~43007/48000 | Train loss: 0.1151\n",
      "Epoch: 012/015 | Batch 43008~43519/48000 | Train loss: 0.1038\n",
      "Epoch: 012/015 | Batch 43520~44031/48000 | Train loss: 0.1039\n",
      "Epoch: 012/015 | Batch 44032~44543/48000 | Train loss: 0.1043\n",
      "Epoch: 012/015 | Batch 44544~45055/48000 | Train loss: 0.1045\n",
      "Epoch: 012/015 | Batch 45056~45567/48000 | Train loss: 0.1044\n",
      "Epoch: 012/015 | Batch 45568~46079/48000 | Train loss: 0.1119\n",
      "Epoch: 012/015 | Batch 46080~46591/48000 | Train loss: 0.1254\n",
      "Epoch: 012/015 | Batch 46592~47103/48000 | Train loss: 0.1205\n",
      "Epoch: 012/015 | Batch 47104~47615/48000 | Train loss: 0.1288\n",
      "Epoch: 012/015 | Batch 47616~48127/48000 | Train loss: 0.1115\n",
      "Epoch: 012/015 | Batch 000~511/12000 | Validation loss: 0.1173\n",
      "Epoch: 012/015 | Batch 512~1023/12000 | Validation loss: 0.0848\n",
      "Epoch: 012/015 | Batch 1024~1535/12000 | Validation loss: 0.0953\n",
      "Epoch: 012/015 | Batch 1536~2047/12000 | Validation loss: 0.1147\n",
      "Epoch: 012/015 | Batch 2048~2559/12000 | Validation loss: 0.1312\n",
      "Epoch: 012/015 | Batch 2560~3071/12000 | Validation loss: 0.1175\n",
      "Epoch: 012/015 | Batch 3072~3583/12000 | Validation loss: 0.1429\n",
      "Epoch: 012/015 | Batch 3584~4095/12000 | Validation loss: 0.1193\n",
      "Epoch: 012/015 | Batch 4096~4607/12000 | Validation loss: 0.0936\n",
      "Epoch: 012/015 | Batch 4608~5119/12000 | Validation loss: 0.1384\n",
      "Epoch: 012/015 | Batch 5120~5631/12000 | Validation loss: 0.1761\n",
      "Epoch: 012/015 | Batch 5632~6143/12000 | Validation loss: 0.1434\n",
      "Epoch: 012/015 | Batch 6144~6655/12000 | Validation loss: 0.1244\n",
      "Epoch: 012/015 | Batch 6656~7167/12000 | Validation loss: 0.1727\n",
      "Epoch: 012/015 | Batch 7168~7679/12000 | Validation loss: 0.1455\n",
      "Epoch: 012/015 | Batch 7680~8191/12000 | Validation loss: 0.1076\n",
      "Epoch: 012/015 | Batch 8192~8703/12000 | Validation loss: 0.1325\n",
      "Epoch: 012/015 | Batch 8704~9215/12000 | Validation loss: 0.1290\n",
      "Epoch: 012/015 | Batch 9216~9727/12000 | Validation loss: 0.1079\n",
      "Epoch: 012/015 | Batch 9728~10239/12000 | Validation loss: 0.0845\n",
      "Epoch: 012/015 | Batch 10240~10751/12000 | Validation loss: 0.1441\n",
      "Epoch: 012/015 | Batch 10752~11263/12000 | Validation loss: 0.1284\n",
      "Epoch: 012/015 | Batch 11264~11775/12000 | Validation loss: 0.1074\n",
      "Epoch: 012/015 | Batch 11776~12287/12000 | Validation loss: 0.1453\n",
      "Epoch: 012/015 | Train loss: 0.1153 | Validation loss: 0.1252 \n",
      "Epoch: 012/015 | Train error: 0.0349 | Validation error: 0.0385 \n",
      "Time elapsed: 49.05 min\n",
      "Epoch: 013/015 | Batch 000~511/48000 | Train loss: 0.1311\n",
      "Epoch: 013/015 | Batch 512~1023/48000 | Train loss: 0.1097\n",
      "Epoch: 013/015 | Batch 1024~1535/48000 | Train loss: 0.1077\n",
      "Epoch: 013/015 | Batch 1536~2047/48000 | Train loss: 0.0874\n",
      "Epoch: 013/015 | Batch 2048~2559/48000 | Train loss: 0.1065\n",
      "Epoch: 013/015 | Batch 2560~3071/48000 | Train loss: 0.1547\n",
      "Epoch: 013/015 | Batch 3072~3583/48000 | Train loss: 0.1505\n",
      "Epoch: 013/015 | Batch 3584~4095/48000 | Train loss: 0.1507\n",
      "Epoch: 013/015 | Batch 4096~4607/48000 | Train loss: 0.3204\n",
      "Epoch: 013/015 | Batch 4608~5119/48000 | Train loss: 0.1839\n",
      "Epoch: 013/015 | Batch 5120~5631/48000 | Train loss: 0.1169\n",
      "Epoch: 013/015 | Batch 5632~6143/48000 | Train loss: 0.1248\n",
      "Epoch: 013/015 | Batch 6144~6655/48000 | Train loss: 0.1682\n",
      "Epoch: 013/015 | Batch 6656~7167/48000 | Train loss: 0.0919\n",
      "Epoch: 013/015 | Batch 7168~7679/48000 | Train loss: 0.1159\n",
      "Epoch: 013/015 | Batch 7680~8191/48000 | Train loss: 0.1069\n",
      "Epoch: 013/015 | Batch 8192~8703/48000 | Train loss: 0.0990\n",
      "Epoch: 013/015 | Batch 8704~9215/48000 | Train loss: 0.1275\n",
      "Epoch: 013/015 | Batch 9216~9727/48000 | Train loss: 0.1452\n",
      "Epoch: 013/015 | Batch 9728~10239/48000 | Train loss: 0.1409\n",
      "Epoch: 013/015 | Batch 10240~10751/48000 | Train loss: 0.1315\n",
      "Epoch: 013/015 | Batch 10752~11263/48000 | Train loss: 0.1454\n",
      "Epoch: 013/015 | Batch 11264~11775/48000 | Train loss: 0.1767\n",
      "Epoch: 013/015 | Batch 11776~12287/48000 | Train loss: 0.1566\n",
      "Epoch: 013/015 | Batch 12288~12799/48000 | Train loss: 0.1248\n",
      "Epoch: 013/015 | Batch 12800~13311/48000 | Train loss: 0.1242\n",
      "Epoch: 013/015 | Batch 13312~13823/48000 | Train loss: 0.1304\n",
      "Epoch: 013/015 | Batch 13824~14335/48000 | Train loss: 0.1407\n",
      "Epoch: 013/015 | Batch 14336~14847/48000 | Train loss: 0.1296\n",
      "Epoch: 013/015 | Batch 14848~15359/48000 | Train loss: 0.1198\n",
      "Epoch: 013/015 | Batch 15360~15871/48000 | Train loss: 0.1016\n",
      "Epoch: 013/015 | Batch 15872~16383/48000 | Train loss: 0.1356\n",
      "Epoch: 013/015 | Batch 16384~16895/48000 | Train loss: 0.1221\n",
      "Epoch: 013/015 | Batch 16896~17407/48000 | Train loss: 0.1211\n",
      "Epoch: 013/015 | Batch 17408~17919/48000 | Train loss: 0.1188\n",
      "Epoch: 013/015 | Batch 17920~18431/48000 | Train loss: 0.1105\n",
      "Epoch: 013/015 | Batch 18432~18943/48000 | Train loss: 0.0994\n",
      "Epoch: 013/015 | Batch 18944~19455/48000 | Train loss: 0.1213\n",
      "Epoch: 013/015 | Batch 19456~19967/48000 | Train loss: 0.1248\n",
      "Epoch: 013/015 | Batch 19968~20479/48000 | Train loss: 0.1302\n",
      "Epoch: 013/015 | Batch 20480~20991/48000 | Train loss: 0.0999\n",
      "Epoch: 013/015 | Batch 20992~21503/48000 | Train loss: 0.1139\n",
      "Epoch: 013/015 | Batch 21504~22015/48000 | Train loss: 0.1298\n",
      "Epoch: 013/015 | Batch 22016~22527/48000 | Train loss: 0.1594\n",
      "Epoch: 013/015 | Batch 22528~23039/48000 | Train loss: 0.0848\n",
      "Epoch: 013/015 | Batch 23040~23551/48000 | Train loss: 0.0918\n",
      "Epoch: 013/015 | Batch 23552~24063/48000 | Train loss: 0.1301\n",
      "Epoch: 013/015 | Batch 24064~24575/48000 | Train loss: 0.1122\n",
      "Epoch: 013/015 | Batch 24576~25087/48000 | Train loss: 0.1373\n",
      "Epoch: 013/015 | Batch 25088~25599/48000 | Train loss: 0.0964\n",
      "Epoch: 013/015 | Batch 25600~26111/48000 | Train loss: 0.1181\n",
      "Epoch: 013/015 | Batch 26112~26623/48000 | Train loss: 0.1239\n",
      "Epoch: 013/015 | Batch 26624~27135/48000 | Train loss: 0.1198\n",
      "Epoch: 013/015 | Batch 27136~27647/48000 | Train loss: 0.1133\n",
      "Epoch: 013/015 | Batch 27648~28159/48000 | Train loss: 0.1431\n",
      "Epoch: 013/015 | Batch 28160~28671/48000 | Train loss: 0.1091\n",
      "Epoch: 013/015 | Batch 28672~29183/48000 | Train loss: 0.1327\n",
      "Epoch: 013/015 | Batch 29184~29695/48000 | Train loss: 0.1132\n",
      "Epoch: 013/015 | Batch 29696~30207/48000 | Train loss: 0.0978\n",
      "Epoch: 013/015 | Batch 30208~30719/48000 | Train loss: 0.1210\n",
      "Epoch: 013/015 | Batch 30720~31231/48000 | Train loss: 0.1219\n",
      "Epoch: 013/015 | Batch 31232~31743/48000 | Train loss: 0.1340\n",
      "Epoch: 013/015 | Batch 31744~32255/48000 | Train loss: 0.1110\n",
      "Epoch: 013/015 | Batch 32256~32767/48000 | Train loss: 0.1110\n",
      "Epoch: 013/015 | Batch 32768~33279/48000 | Train loss: 0.1106\n",
      "Epoch: 013/015 | Batch 33280~33791/48000 | Train loss: 0.1284\n",
      "Epoch: 013/015 | Batch 33792~34303/48000 | Train loss: 0.1112\n",
      "Epoch: 013/015 | Batch 34304~34815/48000 | Train loss: 0.0870\n",
      "Epoch: 013/015 | Batch 34816~35327/48000 | Train loss: 0.1677\n",
      "Epoch: 013/015 | Batch 35328~35839/48000 | Train loss: 0.1201\n",
      "Epoch: 013/015 | Batch 35840~36351/48000 | Train loss: 0.1378\n",
      "Epoch: 013/015 | Batch 36352~36863/48000 | Train loss: 0.0919\n",
      "Epoch: 013/015 | Batch 36864~37375/48000 | Train loss: 0.0963\n",
      "Epoch: 013/015 | Batch 37376~37887/48000 | Train loss: 0.0794\n",
      "Epoch: 013/015 | Batch 37888~38399/48000 | Train loss: 0.1118\n",
      "Epoch: 013/015 | Batch 38400~38911/48000 | Train loss: 0.0909\n",
      "Epoch: 013/015 | Batch 38912~39423/48000 | Train loss: 0.1135\n",
      "Epoch: 013/015 | Batch 39424~39935/48000 | Train loss: 0.0956\n",
      "Epoch: 013/015 | Batch 39936~40447/48000 | Train loss: 0.1327\n",
      "Epoch: 013/015 | Batch 40448~40959/48000 | Train loss: 0.1010\n",
      "Epoch: 013/015 | Batch 40960~41471/48000 | Train loss: 0.1069\n",
      "Epoch: 013/015 | Batch 41472~41983/48000 | Train loss: 0.1283\n",
      "Epoch: 013/015 | Batch 41984~42495/48000 | Train loss: 0.0996\n",
      "Epoch: 013/015 | Batch 42496~43007/48000 | Train loss: 0.1031\n",
      "Epoch: 013/015 | Batch 43008~43519/48000 | Train loss: 0.0913\n",
      "Epoch: 013/015 | Batch 43520~44031/48000 | Train loss: 0.1456\n",
      "Epoch: 013/015 | Batch 44032~44543/48000 | Train loss: 0.0919\n",
      "Epoch: 013/015 | Batch 44544~45055/48000 | Train loss: 0.1117\n",
      "Epoch: 013/015 | Batch 45056~45567/48000 | Train loss: 0.1013\n",
      "Epoch: 013/015 | Batch 45568~46079/48000 | Train loss: 0.1180\n",
      "Epoch: 013/015 | Batch 46080~46591/48000 | Train loss: 0.1072\n",
      "Epoch: 013/015 | Batch 46592~47103/48000 | Train loss: 0.1206\n",
      "Epoch: 013/015 | Batch 47104~47615/48000 | Train loss: 0.0904\n",
      "Epoch: 013/015 | Batch 47616~48127/48000 | Train loss: 0.0853\n",
      "Epoch: 013/015 | Batch 000~511/12000 | Validation loss: 0.1115\n",
      "Epoch: 013/015 | Batch 512~1023/12000 | Validation loss: 0.1086\n",
      "Epoch: 013/015 | Batch 1024~1535/12000 | Validation loss: 0.0863\n",
      "Epoch: 013/015 | Batch 1536~2047/12000 | Validation loss: 0.1038\n",
      "Epoch: 013/015 | Batch 2048~2559/12000 | Validation loss: 0.1457\n",
      "Epoch: 013/015 | Batch 2560~3071/12000 | Validation loss: 0.1045\n",
      "Epoch: 013/015 | Batch 3072~3583/12000 | Validation loss: 0.1371\n",
      "Epoch: 013/015 | Batch 3584~4095/12000 | Validation loss: 0.1307\n",
      "Epoch: 013/015 | Batch 4096~4607/12000 | Validation loss: 0.0710\n",
      "Epoch: 013/015 | Batch 4608~5119/12000 | Validation loss: 0.1424\n",
      "Epoch: 013/015 | Batch 5120~5631/12000 | Validation loss: 0.1899\n",
      "Epoch: 013/015 | Batch 5632~6143/12000 | Validation loss: 0.1452\n",
      "Epoch: 013/015 | Batch 6144~6655/12000 | Validation loss: 0.1332\n",
      "Epoch: 013/015 | Batch 6656~7167/12000 | Validation loss: 0.1568\n",
      "Epoch: 013/015 | Batch 7168~7679/12000 | Validation loss: 0.1315\n",
      "Epoch: 013/015 | Batch 7680~8191/12000 | Validation loss: 0.0907\n",
      "Epoch: 013/015 | Batch 8192~8703/12000 | Validation loss: 0.1089\n",
      "Epoch: 013/015 | Batch 8704~9215/12000 | Validation loss: 0.1500\n",
      "Epoch: 013/015 | Batch 9216~9727/12000 | Validation loss: 0.1173\n",
      "Epoch: 013/015 | Batch 9728~10239/12000 | Validation loss: 0.0938\n",
      "Epoch: 013/015 | Batch 10240~10751/12000 | Validation loss: 0.1567\n",
      "Epoch: 013/015 | Batch 10752~11263/12000 | Validation loss: 0.1238\n",
      "Epoch: 013/015 | Batch 11264~11775/12000 | Validation loss: 0.1271\n",
      "Epoch: 013/015 | Batch 11776~12287/12000 | Validation loss: 0.1186\n",
      "Epoch: 013/015 | Train loss: 0.1214 | Validation loss: 0.1244 \n",
      "Epoch: 013/015 | Train error: 0.0372 | Validation error: 0.0377 \n",
      "Time elapsed: 49.13 min\n",
      "Epoch: 014/015 | Batch 000~511/48000 | Train loss: 0.1321\n",
      "Epoch: 014/015 | Batch 512~1023/48000 | Train loss: 0.1351\n",
      "Epoch: 014/015 | Batch 1024~1535/48000 | Train loss: 0.0848\n",
      "Epoch: 014/015 | Batch 1536~2047/48000 | Train loss: 0.0979\n",
      "Epoch: 014/015 | Batch 2048~2559/48000 | Train loss: 0.1008\n",
      "Epoch: 014/015 | Batch 2560~3071/48000 | Train loss: 0.1277\n",
      "Epoch: 014/015 | Batch 3072~3583/48000 | Train loss: 0.1179\n",
      "Epoch: 014/015 | Batch 3584~4095/48000 | Train loss: 0.1243\n",
      "Epoch: 014/015 | Batch 4096~4607/48000 | Train loss: 0.1182\n",
      "Epoch: 014/015 | Batch 4608~5119/48000 | Train loss: 0.0858\n",
      "Epoch: 014/015 | Batch 5120~5631/48000 | Train loss: 0.1028\n",
      "Epoch: 014/015 | Batch 5632~6143/48000 | Train loss: 0.0901\n",
      "Epoch: 014/015 | Batch 6144~6655/48000 | Train loss: 0.0985\n",
      "Epoch: 014/015 | Batch 6656~7167/48000 | Train loss: 0.1245\n",
      "Epoch: 014/015 | Batch 7168~7679/48000 | Train loss: 0.1182\n",
      "Epoch: 014/015 | Batch 7680~8191/48000 | Train loss: 0.0900\n",
      "Epoch: 014/015 | Batch 8192~8703/48000 | Train loss: 0.1066\n",
      "Epoch: 014/015 | Batch 8704~9215/48000 | Train loss: 0.0979\n",
      "Epoch: 014/015 | Batch 9216~9727/48000 | Train loss: 0.1333\n",
      "Epoch: 014/015 | Batch 9728~10239/48000 | Train loss: 0.1185\n",
      "Epoch: 014/015 | Batch 10240~10751/48000 | Train loss: 0.1419\n",
      "Epoch: 014/015 | Batch 10752~11263/48000 | Train loss: 0.0983\n",
      "Epoch: 014/015 | Batch 11264~11775/48000 | Train loss: 0.0724\n",
      "Epoch: 014/015 | Batch 11776~12287/48000 | Train loss: 0.1067\n",
      "Epoch: 014/015 | Batch 12288~12799/48000 | Train loss: 0.1217\n",
      "Epoch: 014/015 | Batch 12800~13311/48000 | Train loss: 0.1308\n",
      "Epoch: 014/015 | Batch 13312~13823/48000 | Train loss: 0.1241\n",
      "Epoch: 014/015 | Batch 13824~14335/48000 | Train loss: 0.1216\n",
      "Epoch: 014/015 | Batch 14336~14847/48000 | Train loss: 0.1623\n",
      "Epoch: 014/015 | Batch 14848~15359/48000 | Train loss: 0.1169\n",
      "Epoch: 014/015 | Batch 15360~15871/48000 | Train loss: 0.0972\n",
      "Epoch: 014/015 | Batch 15872~16383/48000 | Train loss: 0.1031\n",
      "Epoch: 014/015 | Batch 16384~16895/48000 | Train loss: 0.1126\n",
      "Epoch: 014/015 | Batch 16896~17407/48000 | Train loss: 0.0986\n",
      "Epoch: 014/015 | Batch 17408~17919/48000 | Train loss: 0.0981\n",
      "Epoch: 014/015 | Batch 17920~18431/48000 | Train loss: 0.0848\n",
      "Epoch: 014/015 | Batch 18432~18943/48000 | Train loss: 0.0963\n",
      "Epoch: 014/015 | Batch 18944~19455/48000 | Train loss: 0.1124\n",
      "Epoch: 014/015 | Batch 19456~19967/48000 | Train loss: 0.1188\n",
      "Epoch: 014/015 | Batch 19968~20479/48000 | Train loss: 0.0912\n",
      "Epoch: 014/015 | Batch 20480~20991/48000 | Train loss: 0.1004\n",
      "Epoch: 014/015 | Batch 20992~21503/48000 | Train loss: 0.1211\n",
      "Epoch: 014/015 | Batch 21504~22015/48000 | Train loss: 0.1001\n",
      "Epoch: 014/015 | Batch 22016~22527/48000 | Train loss: 0.1129\n",
      "Epoch: 014/015 | Batch 22528~23039/48000 | Train loss: 0.1077\n",
      "Epoch: 014/015 | Batch 23040~23551/48000 | Train loss: 0.1013\n",
      "Epoch: 014/015 | Batch 23552~24063/48000 | Train loss: 0.0815\n",
      "Epoch: 014/015 | Batch 24064~24575/48000 | Train loss: 0.0818\n",
      "Epoch: 014/015 | Batch 24576~25087/48000 | Train loss: 0.1208\n",
      "Epoch: 014/015 | Batch 25088~25599/48000 | Train loss: 0.1027\n",
      "Epoch: 014/015 | Batch 25600~26111/48000 | Train loss: 0.0845\n",
      "Epoch: 014/015 | Batch 26112~26623/48000 | Train loss: 0.1108\n",
      "Epoch: 014/015 | Batch 26624~27135/48000 | Train loss: 0.1618\n",
      "Epoch: 014/015 | Batch 27136~27647/48000 | Train loss: 0.0879\n",
      "Epoch: 014/015 | Batch 27648~28159/48000 | Train loss: 0.1156\n",
      "Epoch: 014/015 | Batch 28160~28671/48000 | Train loss: 0.0956\n",
      "Epoch: 014/015 | Batch 28672~29183/48000 | Train loss: 0.1264\n",
      "Epoch: 014/015 | Batch 29184~29695/48000 | Train loss: 0.0843\n",
      "Epoch: 014/015 | Batch 29696~30207/48000 | Train loss: 0.0970\n",
      "Epoch: 014/015 | Batch 30208~30719/48000 | Train loss: 0.1168\n",
      "Epoch: 014/015 | Batch 30720~31231/48000 | Train loss: 0.0969\n",
      "Epoch: 014/015 | Batch 31232~31743/48000 | Train loss: 0.1114\n",
      "Epoch: 014/015 | Batch 31744~32255/48000 | Train loss: 0.1052\n",
      "Epoch: 014/015 | Batch 32256~32767/48000 | Train loss: 0.0891\n",
      "Epoch: 014/015 | Batch 32768~33279/48000 | Train loss: 0.1097\n",
      "Epoch: 014/015 | Batch 33280~33791/48000 | Train loss: 0.1359\n",
      "Epoch: 014/015 | Batch 33792~34303/48000 | Train loss: 0.1122\n",
      "Epoch: 014/015 | Batch 34304~34815/48000 | Train loss: 0.1030\n",
      "Epoch: 014/015 | Batch 34816~35327/48000 | Train loss: 0.1153\n",
      "Epoch: 014/015 | Batch 35328~35839/48000 | Train loss: 0.0864\n",
      "Epoch: 014/015 | Batch 35840~36351/48000 | Train loss: 0.1200\n",
      "Epoch: 014/015 | Batch 36352~36863/48000 | Train loss: 0.1385\n",
      "Epoch: 014/015 | Batch 36864~37375/48000 | Train loss: 0.1411\n",
      "Epoch: 014/015 | Batch 37376~37887/48000 | Train loss: 0.1024\n",
      "Epoch: 014/015 | Batch 37888~38399/48000 | Train loss: 0.0866\n",
      "Epoch: 014/015 | Batch 38400~38911/48000 | Train loss: 0.1013\n",
      "Epoch: 014/015 | Batch 38912~39423/48000 | Train loss: 0.1236\n",
      "Epoch: 014/015 | Batch 39424~39935/48000 | Train loss: 0.0921\n",
      "Epoch: 014/015 | Batch 39936~40447/48000 | Train loss: 0.1321\n",
      "Epoch: 014/015 | Batch 40448~40959/48000 | Train loss: 0.0879\n",
      "Epoch: 014/015 | Batch 40960~41471/48000 | Train loss: 0.0981\n",
      "Epoch: 014/015 | Batch 41472~41983/48000 | Train loss: 0.1237\n",
      "Epoch: 014/015 | Batch 41984~42495/48000 | Train loss: 0.1039\n",
      "Epoch: 014/015 | Batch 42496~43007/48000 | Train loss: 0.0871\n",
      "Epoch: 014/015 | Batch 43008~43519/48000 | Train loss: 0.0862\n",
      "Epoch: 014/015 | Batch 43520~44031/48000 | Train loss: 0.0973\n",
      "Epoch: 014/015 | Batch 44032~44543/48000 | Train loss: 0.0622\n",
      "Epoch: 014/015 | Batch 44544~45055/48000 | Train loss: 0.1110\n",
      "Epoch: 014/015 | Batch 45056~45567/48000 | Train loss: 0.0890\n",
      "Epoch: 014/015 | Batch 45568~46079/48000 | Train loss: 0.0991\n",
      "Epoch: 014/015 | Batch 46080~46591/48000 | Train loss: 0.1080\n",
      "Epoch: 014/015 | Batch 46592~47103/48000 | Train loss: 0.0725\n",
      "Epoch: 014/015 | Batch 47104~47615/48000 | Train loss: 0.1000\n",
      "Epoch: 014/015 | Batch 47616~48127/48000 | Train loss: 0.1404\n",
      "Epoch: 014/015 | Batch 000~511/12000 | Validation loss: 0.1300\n",
      "Epoch: 014/015 | Batch 512~1023/12000 | Validation loss: 0.1093\n",
      "Epoch: 014/015 | Batch 1024~1535/12000 | Validation loss: 0.0881\n",
      "Epoch: 014/015 | Batch 1536~2047/12000 | Validation loss: 0.1110\n",
      "Epoch: 014/015 | Batch 2048~2559/12000 | Validation loss: 0.1369\n",
      "Epoch: 014/015 | Batch 2560~3071/12000 | Validation loss: 0.1173\n",
      "Epoch: 014/015 | Batch 3072~3583/12000 | Validation loss: 0.1512\n",
      "Epoch: 014/015 | Batch 3584~4095/12000 | Validation loss: 0.1097\n",
      "Epoch: 014/015 | Batch 4096~4607/12000 | Validation loss: 0.0881\n",
      "Epoch: 014/015 | Batch 4608~5119/12000 | Validation loss: 0.1341\n",
      "Epoch: 014/015 | Batch 5120~5631/12000 | Validation loss: 0.1922\n",
      "Epoch: 014/015 | Batch 5632~6143/12000 | Validation loss: 0.1266\n",
      "Epoch: 014/015 | Batch 6144~6655/12000 | Validation loss: 0.1043\n",
      "Epoch: 014/015 | Batch 6656~7167/12000 | Validation loss: 0.1478\n",
      "Epoch: 014/015 | Batch 7168~7679/12000 | Validation loss: 0.1598\n",
      "Epoch: 014/015 | Batch 7680~8191/12000 | Validation loss: 0.0913\n",
      "Epoch: 014/015 | Batch 8192~8703/12000 | Validation loss: 0.1061\n",
      "Epoch: 014/015 | Batch 8704~9215/12000 | Validation loss: 0.1182\n",
      "Epoch: 014/015 | Batch 9216~9727/12000 | Validation loss: 0.1015\n",
      "Epoch: 014/015 | Batch 9728~10239/12000 | Validation loss: 0.0851\n",
      "Epoch: 014/015 | Batch 10240~10751/12000 | Validation loss: 0.1301\n",
      "Epoch: 014/015 | Batch 10752~11263/12000 | Validation loss: 0.1384\n",
      "Epoch: 014/015 | Batch 11264~11775/12000 | Validation loss: 0.1111\n",
      "Epoch: 014/015 | Batch 11776~12287/12000 | Validation loss: 0.1514\n",
      "Epoch: 014/015 | Train loss: 0.1074 | Validation loss: 0.1225 \n",
      "Epoch: 014/015 | Train error: 0.0321 | Validation error: 0.0370 \n",
      "Time elapsed: 49.19 min\n",
      "Epoch: 015/015 | Batch 000~511/48000 | Train loss: 0.1377\n",
      "Epoch: 015/015 | Batch 512~1023/48000 | Train loss: 0.1253\n",
      "Epoch: 015/015 | Batch 1024~1535/48000 | Train loss: 0.1103\n",
      "Epoch: 015/015 | Batch 1536~2047/48000 | Train loss: 0.0886\n",
      "Epoch: 015/015 | Batch 2048~2559/48000 | Train loss: 0.0822\n",
      "Epoch: 015/015 | Batch 2560~3071/48000 | Train loss: 0.0852\n",
      "Epoch: 015/015 | Batch 3072~3583/48000 | Train loss: 0.1466\n",
      "Epoch: 015/015 | Batch 3584~4095/48000 | Train loss: 0.0844\n",
      "Epoch: 015/015 | Batch 4096~4607/48000 | Train loss: 0.0872\n",
      "Epoch: 015/015 | Batch 4608~5119/48000 | Train loss: 0.0842\n",
      "Epoch: 015/015 | Batch 5120~5631/48000 | Train loss: 0.1268\n",
      "Epoch: 015/015 | Batch 5632~6143/48000 | Train loss: 0.0784\n",
      "Epoch: 015/015 | Batch 6144~6655/48000 | Train loss: 0.0940\n",
      "Epoch: 015/015 | Batch 6656~7167/48000 | Train loss: 0.1104\n",
      "Epoch: 015/015 | Batch 7168~7679/48000 | Train loss: 0.1341\n",
      "Epoch: 015/015 | Batch 7680~8191/48000 | Train loss: 0.0578\n",
      "Epoch: 015/015 | Batch 8192~8703/48000 | Train loss: 0.0700\n",
      "Epoch: 015/015 | Batch 8704~9215/48000 | Train loss: 0.0925\n",
      "Epoch: 015/015 | Batch 9216~9727/48000 | Train loss: 0.1113\n",
      "Epoch: 015/015 | Batch 9728~10239/48000 | Train loss: 0.1154\n",
      "Epoch: 015/015 | Batch 10240~10751/48000 | Train loss: 0.0934\n",
      "Epoch: 015/015 | Batch 10752~11263/48000 | Train loss: 0.0861\n",
      "Epoch: 015/015 | Batch 11264~11775/48000 | Train loss: 0.1345\n",
      "Epoch: 015/015 | Batch 11776~12287/48000 | Train loss: 0.0760\n",
      "Epoch: 015/015 | Batch 12288~12799/48000 | Train loss: 0.0958\n",
      "Epoch: 015/015 | Batch 12800~13311/48000 | Train loss: 0.0711\n",
      "Epoch: 015/015 | Batch 13312~13823/48000 | Train loss: 0.1030\n",
      "Epoch: 015/015 | Batch 13824~14335/48000 | Train loss: 0.0708\n",
      "Epoch: 015/015 | Batch 14336~14847/48000 | Train loss: 0.0797\n",
      "Epoch: 015/015 | Batch 14848~15359/48000 | Train loss: 0.1116\n",
      "Epoch: 015/015 | Batch 15360~15871/48000 | Train loss: 0.1343\n",
      "Epoch: 015/015 | Batch 15872~16383/48000 | Train loss: 0.1164\n",
      "Epoch: 015/015 | Batch 16384~16895/48000 | Train loss: 0.1125\n",
      "Epoch: 015/015 | Batch 16896~17407/48000 | Train loss: 0.0626\n",
      "Epoch: 015/015 | Batch 17408~17919/48000 | Train loss: 0.1269\n",
      "Epoch: 015/015 | Batch 17920~18431/48000 | Train loss: 0.0807\n",
      "Epoch: 015/015 | Batch 18432~18943/48000 | Train loss: 0.0808\n",
      "Epoch: 015/015 | Batch 18944~19455/48000 | Train loss: 0.0862\n",
      "Epoch: 015/015 | Batch 19456~19967/48000 | Train loss: 0.0915\n",
      "Epoch: 015/015 | Batch 19968~20479/48000 | Train loss: 0.1007\n",
      "Epoch: 015/015 | Batch 20480~20991/48000 | Train loss: 0.0892\n",
      "Epoch: 015/015 | Batch 20992~21503/48000 | Train loss: 0.1308\n",
      "Epoch: 015/015 | Batch 21504~22015/48000 | Train loss: 0.0810\n",
      "Epoch: 015/015 | Batch 22016~22527/48000 | Train loss: 0.1054\n",
      "Epoch: 015/015 | Batch 22528~23039/48000 | Train loss: 0.1195\n",
      "Epoch: 015/015 | Batch 23040~23551/48000 | Train loss: 0.1122\n",
      "Epoch: 015/015 | Batch 23552~24063/48000 | Train loss: 0.0971\n",
      "Epoch: 015/015 | Batch 24064~24575/48000 | Train loss: 0.0899\n",
      "Epoch: 015/015 | Batch 24576~25087/48000 | Train loss: 0.1204\n",
      "Epoch: 015/015 | Batch 25088~25599/48000 | Train loss: 0.1347\n",
      "Epoch: 015/015 | Batch 25600~26111/48000 | Train loss: 0.0820\n",
      "Epoch: 015/015 | Batch 26112~26623/48000 | Train loss: 0.1205\n",
      "Epoch: 015/015 | Batch 26624~27135/48000 | Train loss: 0.0524\n",
      "Epoch: 015/015 | Batch 27136~27647/48000 | Train loss: 0.0797\n",
      "Epoch: 015/015 | Batch 27648~28159/48000 | Train loss: 0.1193\n",
      "Epoch: 015/015 | Batch 28160~28671/48000 | Train loss: 0.0841\n",
      "Epoch: 015/015 | Batch 28672~29183/48000 | Train loss: 0.0965\n",
      "Epoch: 015/015 | Batch 29184~29695/48000 | Train loss: 0.0895\n",
      "Epoch: 015/015 | Batch 29696~30207/48000 | Train loss: 0.1079\n",
      "Epoch: 015/015 | Batch 30208~30719/48000 | Train loss: 0.0798\n",
      "Epoch: 015/015 | Batch 30720~31231/48000 | Train loss: 0.0990\n",
      "Epoch: 015/015 | Batch 31232~31743/48000 | Train loss: 0.0951\n",
      "Epoch: 015/015 | Batch 31744~32255/48000 | Train loss: 0.0632\n",
      "Epoch: 015/015 | Batch 32256~32767/48000 | Train loss: 0.0607\n",
      "Epoch: 015/015 | Batch 32768~33279/48000 | Train loss: 0.0728\n",
      "Epoch: 015/015 | Batch 33280~33791/48000 | Train loss: 0.0645\n",
      "Epoch: 015/015 | Batch 33792~34303/48000 | Train loss: 0.1218\n",
      "Epoch: 015/015 | Batch 34304~34815/48000 | Train loss: 0.0949\n",
      "Epoch: 015/015 | Batch 34816~35327/48000 | Train loss: 0.1278\n",
      "Epoch: 015/015 | Batch 35328~35839/48000 | Train loss: 0.1026\n",
      "Epoch: 015/015 | Batch 35840~36351/48000 | Train loss: 0.0903\n",
      "Epoch: 015/015 | Batch 36352~36863/48000 | Train loss: 0.0670\n",
      "Epoch: 015/015 | Batch 36864~37375/48000 | Train loss: 0.1089\n",
      "Epoch: 015/015 | Batch 37376~37887/48000 | Train loss: 0.1352\n",
      "Epoch: 015/015 | Batch 37888~38399/48000 | Train loss: 0.1530\n",
      "Epoch: 015/015 | Batch 38400~38911/48000 | Train loss: 0.1142\n",
      "Epoch: 015/015 | Batch 38912~39423/48000 | Train loss: 0.1138\n",
      "Epoch: 015/015 | Batch 39424~39935/48000 | Train loss: 0.0910\n",
      "Epoch: 015/015 | Batch 39936~40447/48000 | Train loss: 0.0731\n",
      "Epoch: 015/015 | Batch 40448~40959/48000 | Train loss: 0.1194\n",
      "Epoch: 015/015 | Batch 40960~41471/48000 | Train loss: 0.1287\n",
      "Epoch: 015/015 | Batch 41472~41983/48000 | Train loss: 0.0867\n",
      "Epoch: 015/015 | Batch 41984~42495/48000 | Train loss: 0.0820\n",
      "Epoch: 015/015 | Batch 42496~43007/48000 | Train loss: 0.1088\n",
      "Epoch: 015/015 | Batch 43008~43519/48000 | Train loss: 0.1512\n",
      "Epoch: 015/015 | Batch 43520~44031/48000 | Train loss: 0.0829\n",
      "Epoch: 015/015 | Batch 44032~44543/48000 | Train loss: 0.1145\n",
      "Epoch: 015/015 | Batch 44544~45055/48000 | Train loss: 0.0725\n",
      "Epoch: 015/015 | Batch 45056~45567/48000 | Train loss: 0.1237\n",
      "Epoch: 015/015 | Batch 45568~46079/48000 | Train loss: 0.0917\n",
      "Epoch: 015/015 | Batch 46080~46591/48000 | Train loss: 0.0874\n",
      "Epoch: 015/015 | Batch 46592~47103/48000 | Train loss: 0.1228\n",
      "Epoch: 015/015 | Batch 47104~47615/48000 | Train loss: 0.0974\n",
      "Epoch: 015/015 | Batch 47616~48127/48000 | Train loss: 0.1099\n",
      "Epoch: 015/015 | Batch 000~511/12000 | Validation loss: 0.1181\n",
      "Epoch: 015/015 | Batch 512~1023/12000 | Validation loss: 0.1009\n",
      "Epoch: 015/015 | Batch 1024~1535/12000 | Validation loss: 0.0847\n",
      "Epoch: 015/015 | Batch 1536~2047/12000 | Validation loss: 0.0952\n",
      "Epoch: 015/015 | Batch 2048~2559/12000 | Validation loss: 0.1258\n",
      "Epoch: 015/015 | Batch 2560~3071/12000 | Validation loss: 0.0876\n",
      "Epoch: 015/015 | Batch 3072~3583/12000 | Validation loss: 0.1262\n",
      "Epoch: 015/015 | Batch 3584~4095/12000 | Validation loss: 0.1086\n",
      "Epoch: 015/015 | Batch 4096~4607/12000 | Validation loss: 0.0777\n",
      "Epoch: 015/015 | Batch 4608~5119/12000 | Validation loss: 0.1167\n",
      "Epoch: 015/015 | Batch 5120~5631/12000 | Validation loss: 0.1633\n",
      "Epoch: 015/015 | Batch 5632~6143/12000 | Validation loss: 0.1235\n",
      "Epoch: 015/015 | Batch 6144~6655/12000 | Validation loss: 0.1012\n",
      "Epoch: 015/015 | Batch 6656~7167/12000 | Validation loss: 0.1519\n",
      "Epoch: 015/015 | Batch 7168~7679/12000 | Validation loss: 0.1101\n",
      "Epoch: 015/015 | Batch 7680~8191/12000 | Validation loss: 0.0799\n",
      "Epoch: 015/015 | Batch 8192~8703/12000 | Validation loss: 0.1018\n",
      "Epoch: 015/015 | Batch 8704~9215/12000 | Validation loss: 0.1228\n",
      "Epoch: 015/015 | Batch 9216~9727/12000 | Validation loss: 0.1065\n",
      "Epoch: 015/015 | Batch 9728~10239/12000 | Validation loss: 0.0828\n",
      "Epoch: 015/015 | Batch 10240~10751/12000 | Validation loss: 0.1334\n",
      "Epoch: 015/015 | Batch 10752~11263/12000 | Validation loss: 0.1404\n",
      "Epoch: 015/015 | Batch 11264~11775/12000 | Validation loss: 0.1173\n",
      "Epoch: 015/015 | Batch 11776~12287/12000 | Validation loss: 0.1553\n",
      "Epoch: 015/015 | Train loss: 0.0995 | Validation loss: 0.1138 \n",
      "Epoch: 015/015 | Train error: 0.0306 | Validation error: 0.0348 \n",
      "Time elapsed: 49.39 min\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "optimizer = SgdDecay(model_paras_list, lr=lr)\n",
    "optimizers = [optimizer]\n",
    "\n",
    "num_epochs = 15\n",
    "batch_size = 512\n",
    "\n",
    "output = trainer_multiclass(\n",
    "    lenet5_model,\n",
    "    optimizers,\n",
    "    criterion,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    num_epochs,\n",
    "    batch_size,\n",
    "    print_all=True,\n",
    ")\n",
    "\n",
    "training_losses, training_errors, validation_losses, validation_errors = output\n",
    "\n",
    "# benchmark \n",
    "# Epoch: 015/015 | Train loss: 0.0995 | Validation loss: 0.1138 \n",
    "# Epoch: 015/015 | Train error: 0.0306 | Validation error: 0.0348 \n",
    "\n",
    "# this implementation has achieved virtually same level of accuracy as the Pytorch implememntation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
